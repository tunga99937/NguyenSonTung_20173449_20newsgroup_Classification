{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân loại tập văn bản 20newsgroup với mạng ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các công việc cần thực hiện:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xử lý tập dữ liệu 20newsgroup, trong đó:\n",
    "  + Phân tách dữ liệu thành 2 tập để phục vụ do việc học và kiểm thử\n",
    "  + Lưu dữ liệu vào các danh sách để có thể xử lý được (20newsgroups_train.data và 20newsgroups_test.data)\n",
    "  + Tạo danh mục các từ \n",
    "- Xây dựng mạng neural:\n",
    "  + Sử dụng thư viện pytorch\n",
    "  + Mạng neural feed-forward gồm 2 tầng\n",
    "- Training mô hình:\n",
    "  + Thực hiện training 15 lần\n",
    "  + Tập train được chia thành các gói ứng với các đầu vào (input)\n",
    "  + Sử dụng hàm loss: CrossEntropy, hàm tối ưu : Adaptive Moment Estimation (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import Bunch\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo tập train và tập test\n",
    "os.makedirs('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train')\n",
    "os.makedirs('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo các thư mục con trong tập train và test\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20news-18828'):    \n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    os.makedirs(sub_directory_train)\n",
    "    os.makedirs(sub_directory_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy các file sang folder train và test\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20news-18828'):\n",
    "    sub_directory = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20news-18828' + '\\\\' + i\n",
    "    folder_length = len(os.listdir(sub_directory))\n",
    "    \n",
    "    train_length = round(folder_length * 0.6)   # Độ dài tập train (60%)\n",
    "    test_length = folder_length - train_length  # Độ dài tập test  (40%)\n",
    "    \n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    \n",
    "    for news_file in os.listdir(sub_directory)[:train_length]:\n",
    "        news_file_src = sub_directory + '\\\\' + news_file\n",
    "        shutil.copy(news_file_src, sub_directory_train)\n",
    "        \n",
    "    for news_file in os.listdir(sub_directory)[train_length:]:\n",
    "        news_file_src = sub_directory + '\\\\' + news_file\n",
    "        shutil.copy(news_file_src, sub_directory_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo file label cho tập train\n",
    "f = open(\"E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\train_label.txt\",\"w+\")\n",
    "\n",
    "label = 0\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train'):\n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_train):\n",
    "        f.write(\"%d\\n\" %label)\n",
    "    label += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo file label cho tập test\n",
    "f = open(\"E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\test_label.txt\",\"w+\")\n",
    "\n",
    "label = 0\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test'):\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_test):\n",
    "        f.write(\"%d\\n\" %label)\n",
    "    label += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = Bunch()\n",
    "newsgroups_train.data = []\n",
    "\n",
    "newsgroups_test = Bunch()\n",
    "newsgroups_test.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa dữ diệu từ file tập train vào newsgroups_train.data\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train'):\n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_train):\n",
    "        news_file_path = sub_directory_train + '\\\\' + news_file\n",
    "        with open(news_file_path) as f:\n",
    "            news = f.read()\n",
    "            newsgroups_train.data.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11296"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa dữ diệu từ file tập test vào newsgroups_test.data\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test'):\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_test):\n",
    "        news_file_path = sub_directory_test + '\\\\' + news_file\n",
    "        with open(news_file_path) as f:\n",
    "            news = f.read()\n",
    "            newsgroups_test.data.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa các nhãn lớp (chủ đề) của các file train vào newsgroups_train.target\n",
    "newsgroups_train.target = []\n",
    "\n",
    "with open('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\train_label.txt') as f :\n",
    "    all_the_lines = f.readlines()\n",
    "    for i in all_the_lines: \n",
    "        i = int(i)\n",
    "        newsgroups_train.target.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11296\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa các nhãn lớp (chủ đề) của các file test vào newsgroups_test.target\n",
    "newsgroups_test.target = []\n",
    "\n",
    "with open('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\test_label.txt') as f :\n",
    "    all_the_lines = f.readlines()\n",
    "    for i in all_the_lines:         \n",
    "        i = int(i)\n",
    "        newsgroups_test.target.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Làm xáo trộn tập train\n",
    "mapIndexPosition = list(zip(newsgroups_train.data, newsgroups_train.target))\n",
    "random.shuffle(mapIndexPosition)\n",
    "newsgroups_train.data, newsgroups_train.target = zip(*mapIndexPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Làm xáo trộn tập test\n",
    "mapIndexPosition = list(zip(newsgroups_test.data, newsgroups_test.target))\n",
    "random.shuffle(mapIndexPosition)\n",
    "newsgroups_test.data, newsgroups_test.target = zip(*mapIndexPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170781"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hàm tạo danh sách các từ\n",
    "def get_word_index(df1, df2):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vocab = {}\n",
    "    texts = ''\n",
    "    for text in df1.data:\n",
    "        texts = texts + text\n",
    "    for text in df2.data:\n",
    "        texts = texts + text\n",
    "        \n",
    "    vectorizer.fit([texts])\n",
    "    vocab.update(vectorizer.vocabulary_)\n",
    "    return vocab\n",
    "wordindex = get_word_index(newsgroups_train, newsgroups_test)\n",
    "total_words = len(wordindex)\n",
    "\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm tạo các gói(batch) để làm đầu vào(input) khi training cũng như khi test\n",
    "def get_batch(df,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df.data[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df.target[i*batch_size:i*batch_size+batch_size]\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "        processed_text = text.lower()     \n",
    "        processed_text = re.sub('[^a-z]+', \" \", processed_text)\n",
    "        \n",
    "        for word in processed_text.split(' '):\n",
    "            if word in wordindex:\n",
    "                layer[wordindex[word]] += 1     # Có thể coi layer ở đây là một feature vector của văn bản với số chiều là tổng\n",
    "                                                # số từ trong từ điển. Mỗi phần tử trong layer đại diện cho số từ tương ứng  \n",
    "                                                # xuất hiện trong văn bản đó.\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        index_y = -1\n",
    "        for j in range(20):\n",
    "            if category == j:\n",
    "                index_y = j\n",
    "                break\n",
    "    \n",
    "        results.append(index_y)\n",
    "\n",
    "\n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các tham số\n",
    "learning_rate = 0.01\n",
    "num_epochs = 15        #Số lần học\n",
    "batch_size = 150       #Kích thước các gói \n",
    "display_step = 1\n",
    "\n",
    "# Tham số của mạng(network)\n",
    "hidden_size = 100        # Kích thước của các tầng trong mạng\n",
    "input_size = total_words # Tổng số lượng các từ\n",
    "num_classes = 20         # Tổng số nhãn lớp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNet(nn.Module):\n",
    "     def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(OurNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "     def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OurNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [4/75], Loss: 3.3129\n",
      "Epoch [1/15], Step [8/75], Loss: 2.7409\n",
      "Epoch [1/15], Step [12/75], Loss: 2.1095\n",
      "Epoch [1/15], Step [16/75], Loss: 1.6624\n",
      "Epoch [1/15], Step [20/75], Loss: 1.4302\n",
      "Epoch [1/15], Step [24/75], Loss: 1.1345\n",
      "Epoch [1/15], Step [28/75], Loss: 0.8080\n",
      "Epoch [1/15], Step [32/75], Loss: 0.8006\n",
      "Epoch [1/15], Step [36/75], Loss: 0.6691\n",
      "Epoch [1/15], Step [40/75], Loss: 0.5833\n",
      "Epoch [1/15], Step [44/75], Loss: 0.6409\n",
      "Epoch [1/15], Step [48/75], Loss: 0.3707\n",
      "Epoch [1/15], Step [52/75], Loss: 0.5521\n",
      "Epoch [1/15], Step [56/75], Loss: 0.4628\n",
      "Epoch [1/15], Step [60/75], Loss: 0.5137\n",
      "Epoch [1/15], Step [64/75], Loss: 0.5022\n",
      "Epoch [1/15], Step [68/75], Loss: 0.5191\n",
      "Epoch [1/15], Step [72/75], Loss: 0.6529\n",
      "Epoch [1/15], Step [76/75], Loss: 0.2772\n",
      "Epoch [2/15], Step [4/75], Loss: 0.2272\n",
      "Epoch [2/15], Step [8/75], Loss: 0.3096\n",
      "Epoch [2/15], Step [12/75], Loss: 0.2164\n",
      "Epoch [2/15], Step [16/75], Loss: 0.1217\n",
      "Epoch [2/15], Step [20/75], Loss: 0.2319\n",
      "Epoch [2/15], Step [24/75], Loss: 0.2621\n",
      "Epoch [2/15], Step [28/75], Loss: 0.1505\n",
      "Epoch [2/15], Step [32/75], Loss: 0.0689\n",
      "Epoch [2/15], Step [36/75], Loss: 0.0735\n",
      "Epoch [2/15], Step [40/75], Loss: 0.0693\n",
      "Epoch [2/15], Step [44/75], Loss: 0.0710\n",
      "Epoch [2/15], Step [48/75], Loss: 0.0590\n",
      "Epoch [2/15], Step [52/75], Loss: 0.0556\n",
      "Epoch [2/15], Step [56/75], Loss: 0.0299\n",
      "Epoch [2/15], Step [60/75], Loss: 0.0567\n",
      "Epoch [2/15], Step [64/75], Loss: 0.1000\n",
      "Epoch [2/15], Step [68/75], Loss: 0.1549\n",
      "Epoch [2/15], Step [72/75], Loss: 0.1656\n",
      "Epoch [2/15], Step [76/75], Loss: 0.0098\n",
      "Epoch [3/15], Step [4/75], Loss: 0.0126\n",
      "Epoch [3/15], Step [8/75], Loss: 0.0582\n",
      "Epoch [3/15], Step [12/75], Loss: 1.2611\n",
      "Epoch [3/15], Step [16/75], Loss: 0.0811\n",
      "Epoch [3/15], Step [20/75], Loss: 0.0517\n",
      "Epoch [3/15], Step [24/75], Loss: 0.0435\n",
      "Epoch [3/15], Step [28/75], Loss: 0.0088\n",
      "Epoch [3/15], Step [32/75], Loss: 0.0500\n",
      "Epoch [3/15], Step [36/75], Loss: 0.1114\n",
      "Epoch [3/15], Step [40/75], Loss: 0.0528\n",
      "Epoch [3/15], Step [44/75], Loss: 0.0316\n",
      "Epoch [3/15], Step [48/75], Loss: 0.0069\n",
      "Epoch [3/15], Step [52/75], Loss: 0.0166\n",
      "Epoch [3/15], Step [56/75], Loss: 0.0358\n",
      "Epoch [3/15], Step [60/75], Loss: 0.0111\n",
      "Epoch [3/15], Step [64/75], Loss: 0.0514\n",
      "Epoch [3/15], Step [68/75], Loss: 0.0097\n",
      "Epoch [3/15], Step [72/75], Loss: 0.5146\n",
      "Epoch [3/15], Step [76/75], Loss: 0.0352\n",
      "Epoch [4/15], Step [4/75], Loss: 0.0168\n",
      "Epoch [4/15], Step [8/75], Loss: 0.1398\n",
      "Epoch [4/15], Step [12/75], Loss: 0.0368\n",
      "Epoch [4/15], Step [16/75], Loss: 0.0375\n",
      "Epoch [4/15], Step [20/75], Loss: 0.0132\n",
      "Epoch [4/15], Step [24/75], Loss: 0.0857\n",
      "Epoch [4/15], Step [28/75], Loss: 0.0581\n",
      "Epoch [4/15], Step [32/75], Loss: 0.0178\n",
      "Epoch [4/15], Step [36/75], Loss: 0.0187\n",
      "Epoch [4/15], Step [40/75], Loss: 0.0130\n",
      "Epoch [4/15], Step [44/75], Loss: 0.0329\n",
      "Epoch [4/15], Step [48/75], Loss: 0.0063\n",
      "Epoch [4/15], Step [52/75], Loss: 0.0199\n",
      "Epoch [4/15], Step [56/75], Loss: 0.0429\n",
      "Epoch [4/15], Step [60/75], Loss: 0.0058\n",
      "Epoch [4/15], Step [64/75], Loss: 0.0101\n",
      "Epoch [4/15], Step [68/75], Loss: 0.0372\n",
      "Epoch [4/15], Step [72/75], Loss: 0.0271\n",
      "Epoch [4/15], Step [76/75], Loss: 0.0006\n",
      "Epoch [5/15], Step [4/75], Loss: 0.1407\n",
      "Epoch [5/15], Step [8/75], Loss: 0.5980\n",
      "Epoch [5/15], Step [12/75], Loss: 0.1312\n",
      "Epoch [5/15], Step [16/75], Loss: 0.1268\n",
      "Epoch [5/15], Step [20/75], Loss: 0.0164\n",
      "Epoch [5/15], Step [24/75], Loss: 0.6140\n",
      "Epoch [5/15], Step [28/75], Loss: 0.1510\n",
      "Epoch [5/15], Step [32/75], Loss: 0.0117\n",
      "Epoch [5/15], Step [36/75], Loss: 0.0256\n",
      "Epoch [5/15], Step [40/75], Loss: 0.1757\n",
      "Epoch [5/15], Step [44/75], Loss: 0.1731\n",
      "Epoch [5/15], Step [48/75], Loss: 0.0023\n",
      "Epoch [5/15], Step [52/75], Loss: 0.0089\n",
      "Epoch [5/15], Step [56/75], Loss: 0.0189\n",
      "Epoch [5/15], Step [60/75], Loss: 0.0380\n",
      "Epoch [5/15], Step [64/75], Loss: 0.2091\n",
      "Epoch [5/15], Step [68/75], Loss: 0.0027\n",
      "Epoch [5/15], Step [72/75], Loss: 0.1435\n",
      "Epoch [5/15], Step [76/75], Loss: 0.0006\n",
      "Epoch [6/15], Step [4/75], Loss: 0.0404\n",
      "Epoch [6/15], Step [8/75], Loss: 0.2102\n",
      "Epoch [6/15], Step [12/75], Loss: 0.0129\n",
      "Epoch [6/15], Step [16/75], Loss: 0.4250\n",
      "Epoch [6/15], Step [20/75], Loss: 0.1144\n",
      "Epoch [6/15], Step [24/75], Loss: 0.0387\n",
      "Epoch [6/15], Step [28/75], Loss: 0.0040\n",
      "Epoch [6/15], Step [32/75], Loss: 0.0213\n",
      "Epoch [6/15], Step [36/75], Loss: 0.0650\n",
      "Epoch [6/15], Step [40/75], Loss: 0.0821\n",
      "Epoch [6/15], Step [44/75], Loss: 0.0597\n",
      "Epoch [6/15], Step [48/75], Loss: 0.0263\n",
      "Epoch [6/15], Step [52/75], Loss: 0.2437\n",
      "Epoch [6/15], Step [56/75], Loss: 0.0881\n",
      "Epoch [6/15], Step [60/75], Loss: 0.1873\n",
      "Epoch [6/15], Step [64/75], Loss: 0.0334\n",
      "Epoch [6/15], Step [68/75], Loss: 0.1775\n",
      "Epoch [6/15], Step [72/75], Loss: 0.1588\n",
      "Epoch [6/15], Step [76/75], Loss: 0.0010\n",
      "Epoch [7/15], Step [4/75], Loss: 0.0868\n",
      "Epoch [7/15], Step [8/75], Loss: 0.0846\n",
      "Epoch [7/15], Step [12/75], Loss: 0.1176\n",
      "Epoch [7/15], Step [16/75], Loss: 0.3065\n",
      "Epoch [7/15], Step [20/75], Loss: 0.0482\n",
      "Epoch [7/15], Step [24/75], Loss: 0.0042\n",
      "Epoch [7/15], Step [28/75], Loss: 0.1559\n",
      "Epoch [7/15], Step [32/75], Loss: 0.0037\n",
      "Epoch [7/15], Step [36/75], Loss: 0.0578\n",
      "Epoch [7/15], Step [40/75], Loss: 0.0217\n",
      "Epoch [7/15], Step [44/75], Loss: 0.2752\n",
      "Epoch [7/15], Step [48/75], Loss: 0.0192\n",
      "Epoch [7/15], Step [52/75], Loss: 0.2311\n",
      "Epoch [7/15], Step [56/75], Loss: 0.0394\n",
      "Epoch [7/15], Step [60/75], Loss: 0.0339\n",
      "Epoch [7/15], Step [64/75], Loss: 0.0130\n",
      "Epoch [7/15], Step [68/75], Loss: 0.3897\n",
      "Epoch [7/15], Step [72/75], Loss: 0.1911\n",
      "Epoch [7/15], Step [76/75], Loss: 0.0007\n",
      "Epoch [8/15], Step [4/75], Loss: 0.1113\n",
      "Epoch [8/15], Step [8/75], Loss: 0.1895\n",
      "Epoch [8/15], Step [12/75], Loss: 0.0263\n",
      "Epoch [8/15], Step [16/75], Loss: 0.0261\n",
      "Epoch [8/15], Step [20/75], Loss: 0.1757\n",
      "Epoch [8/15], Step [24/75], Loss: 0.0037\n",
      "Epoch [8/15], Step [28/75], Loss: 0.0077\n",
      "Epoch [8/15], Step [32/75], Loss: 0.0933\n",
      "Epoch [8/15], Step [36/75], Loss: 0.0286\n",
      "Epoch [8/15], Step [40/75], Loss: 0.0389\n",
      "Epoch [8/15], Step [44/75], Loss: 0.0299\n",
      "Epoch [8/15], Step [48/75], Loss: 0.0081\n",
      "Epoch [8/15], Step [52/75], Loss: 0.0029\n",
      "Epoch [8/15], Step [56/75], Loss: 0.1601\n",
      "Epoch [8/15], Step [60/75], Loss: 0.1849\n",
      "Epoch [8/15], Step [64/75], Loss: 0.0348\n",
      "Epoch [8/15], Step [68/75], Loss: 0.0129\n",
      "Epoch [8/15], Step [72/75], Loss: 0.0431\n",
      "Epoch [8/15], Step [76/75], Loss: 0.0542\n",
      "Epoch [9/15], Step [4/75], Loss: 0.2735\n",
      "Epoch [9/15], Step [8/75], Loss: 0.4044\n",
      "Epoch [9/15], Step [12/75], Loss: 0.0210\n",
      "Epoch [9/15], Step [16/75], Loss: 0.0391\n",
      "Epoch [9/15], Step [20/75], Loss: 0.1394\n",
      "Epoch [9/15], Step [24/75], Loss: 0.0321\n",
      "Epoch [9/15], Step [28/75], Loss: 0.1708\n",
      "Epoch [9/15], Step [32/75], Loss: 0.0054\n",
      "Epoch [9/15], Step [36/75], Loss: 0.0308\n",
      "Epoch [9/15], Step [40/75], Loss: 0.6575\n",
      "Epoch [9/15], Step [44/75], Loss: 0.0492\n",
      "Epoch [9/15], Step [48/75], Loss: 0.0033\n",
      "Epoch [9/15], Step [52/75], Loss: 0.0028\n",
      "Epoch [9/15], Step [56/75], Loss: 0.0016\n",
      "Epoch [9/15], Step [60/75], Loss: 0.0028\n",
      "Epoch [9/15], Step [64/75], Loss: 0.0014\n",
      "Epoch [9/15], Step [68/75], Loss: 0.0044\n",
      "Epoch [9/15], Step [72/75], Loss: 0.0348\n",
      "Epoch [9/15], Step [76/75], Loss: 0.0387\n",
      "Epoch [10/15], Step [4/75], Loss: 0.0004\n",
      "Epoch [10/15], Step [8/75], Loss: 0.5348\n",
      "Epoch [10/15], Step [12/75], Loss: 0.0185\n",
      "Epoch [10/15], Step [16/75], Loss: 0.1450\n",
      "Epoch [10/15], Step [20/75], Loss: 0.0033\n",
      "Epoch [10/15], Step [24/75], Loss: 0.0349\n",
      "Epoch [10/15], Step [28/75], Loss: 0.0028\n",
      "Epoch [10/15], Step [32/75], Loss: 0.0004\n",
      "Epoch [10/15], Step [36/75], Loss: 0.0583\n",
      "Epoch [10/15], Step [40/75], Loss: 0.1893\n",
      "Epoch [10/15], Step [44/75], Loss: 0.0426\n",
      "Epoch [10/15], Step [48/75], Loss: 0.0492\n",
      "Epoch [10/15], Step [52/75], Loss: 0.0349\n",
      "Epoch [10/15], Step [56/75], Loss: 0.0014\n",
      "Epoch [10/15], Step [60/75], Loss: 0.0068\n",
      "Epoch [10/15], Step [64/75], Loss: 0.0409\n",
      "Epoch [10/15], Step [68/75], Loss: 0.0018\n",
      "Epoch [10/15], Step [72/75], Loss: 0.0026\n",
      "Epoch [10/15], Step [76/75], Loss: 0.0003\n",
      "Epoch [11/15], Step [4/75], Loss: 0.0010\n",
      "Epoch [11/15], Step [8/75], Loss: 0.0549\n",
      "Epoch [11/15], Step [12/75], Loss: 0.1052\n",
      "Epoch [11/15], Step [16/75], Loss: 0.0264\n",
      "Epoch [11/15], Step [20/75], Loss: 0.0006\n",
      "Epoch [11/15], Step [24/75], Loss: 0.0045\n",
      "Epoch [11/15], Step [28/75], Loss: 0.6034\n",
      "Epoch [11/15], Step [32/75], Loss: 0.1352\n",
      "Epoch [11/15], Step [36/75], Loss: 0.0323\n",
      "Epoch [11/15], Step [40/75], Loss: 0.0371\n",
      "Epoch [11/15], Step [44/75], Loss: 0.0622\n",
      "Epoch [11/15], Step [48/75], Loss: 0.0931\n",
      "Epoch [11/15], Step [52/75], Loss: 0.0195\n",
      "Epoch [11/15], Step [56/75], Loss: 0.0070\n",
      "Epoch [11/15], Step [60/75], Loss: 0.2237\n",
      "Epoch [11/15], Step [64/75], Loss: 0.0110\n",
      "Epoch [11/15], Step [68/75], Loss: 0.0078\n",
      "Epoch [11/15], Step [72/75], Loss: 0.0134\n",
      "Epoch [11/15], Step [76/75], Loss: 0.0192\n",
      "Epoch [12/15], Step [4/75], Loss: 0.0192\n",
      "Epoch [12/15], Step [8/75], Loss: 0.0895\n",
      "Epoch [12/15], Step [12/75], Loss: 0.0224\n",
      "Epoch [12/15], Step [16/75], Loss: 0.0042\n",
      "Epoch [12/15], Step [20/75], Loss: 0.0113\n",
      "Epoch [12/15], Step [24/75], Loss: 0.0011\n",
      "Epoch [12/15], Step [28/75], Loss: 0.0906\n",
      "Epoch [12/15], Step [32/75], Loss: 0.0094\n",
      "Epoch [12/15], Step [36/75], Loss: 0.0015\n",
      "Epoch [12/15], Step [40/75], Loss: 0.0003\n",
      "Epoch [12/15], Step [44/75], Loss: 0.0301\n",
      "Epoch [12/15], Step [48/75], Loss: 0.0004\n",
      "Epoch [12/15], Step [52/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [56/75], Loss: 0.0149\n",
      "Epoch [12/15], Step [60/75], Loss: 0.0009\n",
      "Epoch [12/15], Step [64/75], Loss: 0.0341\n",
      "Epoch [12/15], Step [68/75], Loss: 0.0003\n",
      "Epoch [12/15], Step [72/75], Loss: 0.0014\n",
      "Epoch [12/15], Step [76/75], Loss: 0.0000\n",
      "Epoch [13/15], Step [4/75], Loss: 0.1045\n",
      "Epoch [13/15], Step [8/75], Loss: 0.0502\n",
      "Epoch [13/15], Step [12/75], Loss: 0.0101\n",
      "Epoch [13/15], Step [16/75], Loss: 0.0699\n",
      "Epoch [13/15], Step [20/75], Loss: 0.0004\n",
      "Epoch [13/15], Step [24/75], Loss: 0.0015\n",
      "Epoch [13/15], Step [28/75], Loss: 0.0012\n",
      "Epoch [13/15], Step [32/75], Loss: 0.0016\n",
      "Epoch [13/15], Step [36/75], Loss: 0.0002\n",
      "Epoch [13/15], Step [40/75], Loss: 0.0192\n",
      "Epoch [13/15], Step [44/75], Loss: 0.0672\n",
      "Epoch [13/15], Step [48/75], Loss: 0.1179\n",
      "Epoch [13/15], Step [52/75], Loss: 0.0001\n",
      "Epoch [13/15], Step [56/75], Loss: 0.0003\n",
      "Epoch [13/15], Step [60/75], Loss: 0.0004\n",
      "Epoch [13/15], Step [64/75], Loss: 0.0012\n",
      "Epoch [13/15], Step [68/75], Loss: 0.0002\n",
      "Epoch [13/15], Step [72/75], Loss: 0.0015\n",
      "Epoch [13/15], Step [76/75], Loss: 0.2978\n",
      "Epoch [14/15], Step [4/75], Loss: 0.0450\n",
      "Epoch [14/15], Step [8/75], Loss: 0.6726\n",
      "Epoch [14/15], Step [12/75], Loss: 0.0570\n",
      "Epoch [14/15], Step [16/75], Loss: 0.0621\n",
      "Epoch [14/15], Step [20/75], Loss: 0.0222\n",
      "Epoch [14/15], Step [24/75], Loss: 0.0014\n",
      "Epoch [14/15], Step [28/75], Loss: 0.0023\n",
      "Epoch [14/15], Step [32/75], Loss: 0.0062\n",
      "Epoch [14/15], Step [36/75], Loss: 0.0018\n",
      "Epoch [14/15], Step [40/75], Loss: 0.0969\n",
      "Epoch [14/15], Step [44/75], Loss: 0.0022\n",
      "Epoch [14/15], Step [48/75], Loss: 0.0002\n",
      "Epoch [14/15], Step [52/75], Loss: 0.0010\n",
      "Epoch [14/15], Step [56/75], Loss: 0.0103\n",
      "Epoch [14/15], Step [60/75], Loss: 0.0166\n",
      "Epoch [14/15], Step [64/75], Loss: 0.0096\n",
      "Epoch [14/15], Step [68/75], Loss: 0.0095\n",
      "Epoch [14/15], Step [72/75], Loss: 0.0217\n",
      "Epoch [14/15], Step [76/75], Loss: 0.0000\n",
      "Epoch [15/15], Step [4/75], Loss: 0.0365\n",
      "Epoch [15/15], Step [8/75], Loss: 0.0615\n",
      "Epoch [15/15], Step [12/75], Loss: 0.1694\n",
      "Epoch [15/15], Step [16/75], Loss: 0.2137\n",
      "Epoch [15/15], Step [20/75], Loss: 0.0005\n",
      "Epoch [15/15], Step [24/75], Loss: 0.0003\n",
      "Epoch [15/15], Step [28/75], Loss: 0.0001\n",
      "Epoch [15/15], Step [32/75], Loss: 0.0108\n",
      "Epoch [15/15], Step [36/75], Loss: 0.0029\n",
      "Epoch [15/15], Step [40/75], Loss: 0.0298\n",
      "Epoch [15/15], Step [44/75], Loss: 0.0136\n",
      "Epoch [15/15], Step [48/75], Loss: 0.0000\n",
      "Epoch [15/15], Step [52/75], Loss: 0.0005\n",
      "Epoch [15/15], Step [56/75], Loss: 0.0031\n",
      "Epoch [15/15], Step [60/75], Loss: 0.0001\n",
      "Epoch [15/15], Step [64/75], Loss: 0.0002\n",
      "Epoch [15/15], Step [68/75], Loss: 0.0002\n",
      "Epoch [15/15], Step [72/75], Loss: 0.0276\n",
      "Epoch [15/15], Step [76/75], Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Hàm Loss và Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(newsgroups_train.data)/batch_size)\n",
    "    #  Lặp lại qua từng gói\n",
    "    for i in range(total_batch + 1):\n",
    "        batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()            # Đưa các giá trị gradient về 0\n",
    "        outputs = net(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()                  # Tính toán các gradient  \n",
    "        optimizer.step()                 # Cập nhật các tham số\n",
    "\n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(newsgroups_train.data)//batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test articles: 77 %\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "correct = 0\n",
    "\n",
    "total_test_data = len(newsgroups_test.target)\n",
    "total_batch = int(total_test_data/batch_size)\n",
    "for i in range(total_batch + 1):\n",
    "    batch_x_test,batch_y_test = get_batch(newsgroups_test,i,batch_size)\n",
    "    articles = Variable(torch.FloatTensor(batch_x_test))\n",
    "    labels = torch.LongTensor(batch_y_test)\n",
    "    \n",
    "    outputs = net(articles)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "   \n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the network on the test articles: %d %%' % (100 * correct / total_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
