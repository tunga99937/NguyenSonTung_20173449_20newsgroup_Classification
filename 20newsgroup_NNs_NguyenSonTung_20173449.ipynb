{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân loại tập văn bản 20newsgroup với mạng ANN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các công việc cần thực hiện:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xử lý tập dữ liệu 20newsgroup, trong đó:\n",
    "  + Phân tách dữ liệu thành 2 tập để phục vụ do việc học và kiểm thử\n",
    "  + Lưu dữ liệu vào các danh sách để có thể xử lý được (20newsgroups_train.data và 20newsgroups_test.data)\n",
    "  + Tạo danh mục các từ \n",
    "- Xây dựng mạng neural:\n",
    "  + Sử dụng thư viện pytorch\n",
    "  + Mạng neural feed-forward gồm 2 tầng\n",
    "- Training mô hình:\n",
    "  + Thực hiện training 15 lần\n",
    "  + Tập train được chia thành các gói ứng với các đầu vào (input)\n",
    "  + Sử dụng hàm loss: CrossEntropy, hàm tối ưu : Adaptive Moment Estimation (Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import Bunch\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo tập train và tập test\n",
    "os.makedirs('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train')\n",
    "os.makedirs('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo các thư mục con trong tập train và test\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20news-18828'):    \n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    os.makedirs(sub_directory_train)\n",
    "    os.makedirs(sub_directory_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy các file sang folder train và test\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20news-18828'):\n",
    "    sub_directory = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20news-18828' + '\\\\' + i\n",
    "    folder_length = len(os.listdir(sub_directory))\n",
    "    \n",
    "    train_length = round(folder_length * 0.6)   # Độ dài tập train (60%)\n",
    "    test_length = folder_length - train_length  # Độ dài tập test  (40%)\n",
    "    \n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    \n",
    "    for news_file in os.listdir(sub_directory)[:train_length]:\n",
    "        news_file_src = sub_directory + '\\\\' + news_file\n",
    "        shutil.copy(news_file_src, sub_directory_train)\n",
    "        \n",
    "    for news_file in os.listdir(sub_directory)[train_length:]:\n",
    "        news_file_src = sub_directory + '\\\\' + news_file\n",
    "        shutil.copy(news_file_src, sub_directory_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo file label cho tập train\n",
    "f = open(\"E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\train_label.txt\",\"w+\")\n",
    "\n",
    "label = 0\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train'):\n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_train):\n",
    "        f.write(\"%d\\n\" %label)\n",
    "    label += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo file label cho tập test\n",
    "f = open(\"E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\test_label.txt\",\"w+\")\n",
    "\n",
    "label = 0\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test'):\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_test):\n",
    "        f.write(\"%d\\n\" %label)\n",
    "    label += 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = Bunch()\n",
    "newsgroups_train.data = []\n",
    "newsgroups_train.target = []\n",
    "\n",
    "newsgroups_test = Bunch()\n",
    "newsgroups_test.data = []\n",
    "newsgroups_test.target = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa dữ diệu từ file tập train vào newsgroups_train.data\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train'):\n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_train):\n",
    "        news_file_path = sub_directory_train + '\\\\' + news_file\n",
    "        with open(news_file_path) as f:\n",
    "            news = f.read()\n",
    "            newsgroups_train.data.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11296"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa dữ diệu từ file tập test vào newsgroups_test.data\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test'):\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_test):\n",
    "        news_file_path = sub_directory_test + '\\\\' + news_file\n",
    "        with open(news_file_path) as f:\n",
    "            news = f.read()\n",
    "            newsgroups_test.data.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa các nhãn lớp (chủ đề) của các file train vào newsgroups_train.target\n",
    "\n",
    "with open('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\train_label.txt') as f :\n",
    "    all_the_lines = f.readlines()\n",
    "    for i in all_the_lines: \n",
    "        i = int(i)\n",
    "        newsgroups_train.target.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11296\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa các nhãn lớp (chủ đề) của các file test vào newsgroups_test.target\n",
    "\n",
    "with open('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\test_label.txt') as f :\n",
    "    all_the_lines = f.readlines()\n",
    "    for i in all_the_lines:         \n",
    "        i = int(i)\n",
    "        newsgroups_test.target.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532\n"
     ]
    }
   ],
   "source": [
    "print(len(newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Làm xáo trộn tập train\n",
    "mapIndexPosition = list(zip(newsgroups_train.data, newsgroups_train.target))\n",
    "random.shuffle(mapIndexPosition)\n",
    "newsgroups_train.data, newsgroups_train.target = zip(*mapIndexPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Làm xáo trộn tập test\n",
    "mapIndexPosition = list(zip(newsgroups_test.data, newsgroups_test.target))\n",
    "random.shuffle(mapIndexPosition)\n",
    "newsgroups_test.data, newsgroups_test.target = zip(*mapIndexPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170781"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hàm tạo danh sách các từ\n",
    "def get_word_index(df1, df2):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vocab = {}\n",
    "    texts = ''\n",
    "    for text in df1.data:\n",
    "        texts = texts + text\n",
    "    for text in df2.data:\n",
    "        texts = texts + text\n",
    "        \n",
    "    vectorizer.fit([texts])\n",
    "    vocab.update(vectorizer.vocabulary_)\n",
    "    return vocab\n",
    "wordindex = get_word_index(newsgroups_train, newsgroups_test)\n",
    "total_words = len(wordindex)\n",
    "\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': 74959,\n",
       " 'eechen': 66729,\n",
       " 'leland': 98987,\n",
       " 'stanford': 144385,\n",
       " 'edu': 66676,\n",
       " 'emery': 67796,\n",
       " 'ethan': 69432,\n",
       " 'chen': 52299,\n",
       " 'subject': 145756,\n",
       " 're': 130962,\n",
       " 'let': 99155,\n",
       " 'talk': 148337,\n",
       " 'phillies': 122938,\n",
       " 'article': 39805,\n",
       " 'as': 39878,\n",
       " 'follows': 73974,\n",
       " 'bml2': 45561,\n",
       " 'ns1': 115859,\n",
       " 'cc': 51147,\n",
       " 'lehigh': 98946,\n",
       " 'brian': 46768,\n",
       " 'michael': 107518,\n",
       " 'lucy': 101472,\n",
       " 'date': 59732,\n",
       " '15': 6049,\n",
       " 'apr': 39045,\n",
       " '93': 29347,\n",
       " '06': 1479,\n",
       " '29': 13735,\n",
       " '05': 1332,\n",
       " 'gmt': 78318,\n",
       " 'organization': 118753,\n",
       " 'university': 155808,\n",
       " 'lines': 99947,\n",
       " 'in': 87625,\n",
       " 'uflkll_00vpcekw15e': 154526,\n",
       " 'andrew': 38001,\n",
       " 'cmu': 54041,\n",
       " 'al1x': 36756,\n",
       " 'amit': 37622,\n",
       " 'likhy': 99805,\n",
       " 'ani': 38113,\n",
       " 'writes': 163978,\n",
       " 'excerpts': 70117,\n",
       " 'netnews': 113979,\n",
       " 'rec': 131239,\n",
       " 'sport': 143556,\n",
       " 'baseball': 42858,\n",
       " 'u96_msopher': 154073,\n",
       " 'vaxc': 158192,\n",
       " 'stevens': 144876,\n",
       " '963': 29997,\n",
       " 'like': 99787,\n",
       " 'this': 150195,\n",
       " 'oh': 117610,\n",
       " 'well': 162017,\n",
       " 'how': 84421,\n",
       " 'do': 63775,\n",
       " 'we': 161793,\n",
       " 'spell': 143273,\n",
       " 'cellar': 51442,\n",
       " 'ninja': 114761,\n",
       " 'jew': 91919,\n",
       " 'are': 39367,\n",
       " 'there': 150000,\n",
       " 'any': 38542,\n",
       " 'philly': 122947,\n",
       " 'fans': 71654,\n",
       " 'who': 162519,\n",
       " 'want': 161303,\n",
       " 'to': 151187,\n",
       " 'put': 127445,\n",
       " 'money': 109675,\n",
       " 'on': 118075,\n",
       " 'that': 149819,\n",
       " 'if': 86572,\n",
       " 'not': 115543,\n",
       " 'stop': 145136,\n",
       " 'your': 168378,\n",
       " 'woofing': 163608,\n",
       " 'ben': 43845,\n",
       " 'rivera': 134036,\n",
       " 'got': 78747,\n",
       " 'hammered': 81312,\n",
       " 'true': 152729,\n",
       " 'last': 98260,\n",
       " 'week': 161899,\n",
       " 'but': 47840,\n",
       " 'tonight': 151406,\n",
       " 'he': 82150,\n",
       " 'pitched': 123550,\n",
       " 'shutout': 140458,\n",
       " 'innings': 88604,\n",
       " 'and': 37948,\n",
       " 'runs': 135761,\n",
       " 'behind': 43652,\n",
       " 'him': 83254,\n",
       " 'why': 162586,\n",
       " 'one': 118109,\n",
       " 'phrase': 123112,\n",
       " 'for': 74079,\n",
       " 'you': 168353,\n",
       " 'fuck': 75220,\n",
       " 'thanks': 149802,\n",
       " 'livesey': 100212,\n",
       " 'solntze': 142542,\n",
       " 'wpd': 163825,\n",
       " 'sgi': 139596,\n",
       " 'com': 54620,\n",
       " 'jon': 92743,\n",
       " 'political': 124511,\n",
       " 'atheists': 40470,\n",
       " '1ql0ajinn2kj': 9702,\n",
       " 'gap': 76481,\n",
       " 'caltech': 50082,\n",
       " 'keith': 94858,\n",
       " 'cco': 51224,\n",
       " 'allan': 37064,\n",
       " 'schneider': 137937,\n",
       " 'kmr4': 96114,\n",
       " 'po': 124327,\n",
       " 'cwru': 58517,\n",
       " 'ryan': 136051,\n",
       " 'chimps': 52478,\n",
       " 'almost': 37213,\n",
       " 'human': 84970,\n",
       " 'does': 63913,\n",
       " 'mean': 106103,\n",
       " 'have': 81899,\n",
       " 'moral': 109898,\n",
       " 'will': 162763,\n",
       " 'must': 111659,\n",
       " 'some': 142583,\n",
       " 'system': 147478,\n",
       " 'they': 150090,\n",
       " 'live': 100197,\n",
       " 'social': 142331,\n",
       " 'groups': 79581,\n",
       " 'so': 142284,\n",
       " 'laws': 98435,\n",
       " 'dictating': 62206,\n",
       " 'undesired': 155444,\n",
       " 'behavior': 43634,\n",
       " 'ah': 36227,\n",
       " 'the': 149840,\n",
       " 'verb': 158607,\n",
       " 'was': 161422,\n",
       " 'warned': 161374,\n",
       " 'about': 34489,\n",
       " 'back': 42167,\n",
       " 'kindergarten': 95578,\n",
       " 'such': 146015,\n",
       " 'donald': 64038,\n",
       " 'dswalker': 64972,\n",
       " 'ebay': 66277,\n",
       " 'sun': 146241,\n",
       " 'don': 64035,\n",
       " 'walker': 161197,\n",
       " 'items': 90347,\n",
       " 'sale': 136959,\n",
       " 'howard': 84425,\n",
       " 'miller': 107945,\n",
       " 'clock': 53823,\n",
       " 'it': 90306,\n",
       " 'chimes': 52473,\n",
       " 'grandfather': 79069,\n",
       " '250': 12751,\n",
       " 'painting': 120576,\n",
       " 'tiger': 150634,\n",
       " 'snow': 142231,\n",
       " 'is': 90026,\n",
       " 'beautiful': 43432,\n",
       " 'looks': 100829,\n",
       " 'can': 50191,\n",
       " 'jump': 93328,\n",
       " 'off': 117447,\n",
       " 'of': 117426,\n",
       " 'canvas': 50334,\n",
       " 'get': 77388,\n",
       " '200': 10873,\n",
       " 'mens': 106626,\n",
       " 'diamond': 62127,\n",
       " 'ring': 133912,\n",
       " 'size': 141170,\n",
       " '10': 3210,\n",
       " '500': 19913,\n",
       " 'rows': 135189,\n",
       " 'diamonds': 62129,\n",
       " '18k': 8010,\n",
       " 'gold': 78526,\n",
       " 'call': 50027,\n",
       " 'or': 118604,\n",
       " 'email': 67674,\n",
       " 'me': 106033,\n",
       " 'hm': 83598,\n",
       " '408': 17830,\n",
       " '263': 13066,\n",
       " '3709': 16333,\n",
       " 'wk': 163237,\n",
       " '276': 13341,\n",
       " '3618': 16211,\n",
       " 'hhenderson': 83022,\n",
       " 'vax': 158181,\n",
       " 'clarku': 53510,\n",
       " 'game': 76404,\n",
       " 'length': 99037,\n",
       " 'braves': 46602,\n",
       " 'update': 156421,\n",
       " 'ecaxron': 66351,\n",
       " 'ariel': 39466,\n",
       " 'lerc': 99105,\n",
       " 'nasa': 113185,\n",
       " 'gov': 78790,\n",
       " 'ron': 134917,\n",
       " 'graham': 79032,\n",
       " 'hesitate': 82812,\n",
       " 'make': 103995,\n",
       " 'assumptions': 40246,\n",
       " 'other': 119140,\n",
       " 'people': 122095,\n",
       " 'when': 162355,\n",
       " 'write': 163971,\n",
       " 'neither': 113816,\n",
       " 'two': 153475,\n",
       " 'sound': 142799,\n",
       " 'though': 150288,\n",
       " 'kids': 95463,\n",
       " 'may': 105221,\n",
       " 'spouses': 143606,\n",
       " 'either': 67156,\n",
       " 'wonder': 163536,\n",
       " 'say': 137508,\n",
       " 'see': 138787,\n",
       " 'having': 81917,\n",
       " 'spouse': 143605,\n",
       " 'would': 163767,\n",
       " 'anything': 38569,\n",
       " 'with': 163125,\n",
       " 'might': 107778,\n",
       " 'brought': 47046,\n",
       " 'wanted': 161306,\n",
       " 'them': 149913,\n",
       " 'home': 83968,\n",
       " 'time': 150711,\n",
       " 'bed': 43487,\n",
       " 'at': 40395,\n",
       " 'reasonable': 131144,\n",
       " 'hour': 84375,\n",
       " 'which': 162394,\n",
       " 'case': 50742,\n",
       " 'probably': 126095,\n",
       " 'decide': 60312,\n",
       " 'take': 148293,\n",
       " 'my': 112127,\n",
       " 'games': 76420,\n",
       " 'weekends': 161904,\n",
       " 'many': 104424,\n",
       " 'else': 67607,\n",
       " 'be': 43321,\n",
       " 'prepared': 125648,\n",
       " 'leave': 98759,\n",
       " 'early': 66161,\n",
       " 'what': 162301,\n",
       " 'later': 98289,\n",
       " 'bring': 46861,\n",
       " 'happens': 81516,\n",
       " 'husband': 85126,\n",
       " 'no': 115120,\n",
       " 'usually': 156883,\n",
       " 'attends': 40639,\n",
       " 'extra': 70631,\n",
       " 'could': 56760,\n",
       " 'past': 121267,\n",
       " 'midnight': 107736,\n",
       " 'even': 69765,\n",
       " 'without': 163152,\n",
       " 'score': 138157,\n",
       " 'beyond': 44206,\n",
       " 'family': 71619,\n",
       " 'night': 114657,\n",
       " 'all': 37058,\n",
       " 'next': 114286,\n",
       " 'day': 59867,\n",
       " 'workday': 163664,\n",
       " 'bag': 42328,\n",
       " 'problem': 126117,\n",
       " 'wife': 162686,\n",
       " 'ballgame': 42488,\n",
       " 'goes': 78490,\n",
       " 'trim': 152531,\n",
       " 'hours': 84379,\n",
       " 'far': 71682,\n",
       " 'bagging': 42339,\n",
       " 'work': 163655,\n",
       " 'really': 131086,\n",
       " 'understand': 155400,\n",
       " 'necessary': 113648,\n",
       " 'unless': 155868,\n",
       " 'going': 78513,\n",
       " 'also': 37283,\n",
       " 'means': 106116,\n",
       " 'consume': 55953,\n",
       " 'vast': 158143,\n",
       " 'quantities': 129066,\n",
       " 'beer': 43554,\n",
       " 'note': 115560,\n",
       " 'guys': 80208,\n",
       " 'think': 150156,\n",
       " 'up': 156396,\n",
       " 'go': 78394,\n",
       " 'after': 35995,\n",
       " 'long': 100776,\n",
       " 'evening': 69769,\n",
       " 'out': 119265,\n",
       " 'then': 149933,\n",
       " 'evenings': 69770,\n",
       " 'during': 65304,\n",
       " 'stuff': 145596,\n",
       " 'enters': 68443,\n",
       " 'into': 89363,\n",
       " 'consideration': 55789,\n",
       " 'care': 50523,\n",
       " 'whether': 162381,\n",
       " 'sped': 143226,\n",
       " 'impo': 87479,\n",
       " 'saying': 137515,\n",
       " 'where': 162360,\n",
       " 'families': 71614,\n",
       " 'look': 100817,\n",
       " 'pay': 121490,\n",
       " 'between': 44177,\n",
       " '00': 0,\n",
       " '20': 10872,\n",
       " 'apiece': 38726,\n",
       " 'tickets': 150571,\n",
       " 'plus': 124140,\n",
       " 'parking': 121024,\n",
       " 'spending': 143290,\n",
       " 'energy': 68210,\n",
       " 'driving': 64727,\n",
       " 'boston': 46176,\n",
       " 'additional': 35317,\n",
       " 'concessions': 55332,\n",
       " 'etc': 69399,\n",
       " 'ballpark': 42506,\n",
       " 'considerable': 55786,\n",
       " 'investment': 89529,\n",
       " 'afford': 35915,\n",
       " 'lot': 100950,\n",
       " 'hence': 82595,\n",
       " 'big': 44542,\n",
       " 'deal': 60153,\n",
       " 'play': 123917,\n",
       " 'an': 37787,\n",
       " 'opera': 118355,\n",
       " 'maybe': 105231,\n",
       " 'more': 109928,\n",
       " 'than': 149790,\n",
       " 'year': 167811,\n",
       " 'interested': 89124,\n",
       " '_die': 32130,\n",
       " 'walkure_': 161208,\n",
       " 'cut': 58391,\n",
       " 'down': 64314,\n",
       " 'convenient': 56200,\n",
       " 'weeknight': 161908,\n",
       " 'version': 158718,\n",
       " 'felt': 72300,\n",
       " 'interfere': 89137,\n",
       " 'life': 99696,\n",
       " 'choose': 52628,\n",
       " 'weekend': 161902,\n",
       " 'heather': 82272,\n",
       " 'kadie': 94188,\n",
       " 'cs': 57844,\n",
       " 'uiuc': 154684,\n",
       " 'carl': 50566,\n",
       " 'organized': 118757,\n",
       " 'lobbying': 100580,\n",
       " 'cryptography': 57809,\n",
       " 'kubo': 96984,\n",
       " 'zariski': 169370,\n",
       " 'harvard': 81754,\n",
       " 'tal': 148310,\n",
       " 'eff': 66808,\n",
       " 'has': 81765,\n",
       " 'been': 43547,\n",
       " 'associated': 40216,\n",
       " 'efforts': 66854,\n",
       " 'prevent': 125848,\n",
       " 'banning': 42623,\n",
       " 'sex': 139478,\n",
       " 'pictures': 123275,\n",
       " 'newsgroups': 114224,\n",
       " 'various': 158088,\n",
       " 'universities': 155806,\n",
       " 'justices': 93415,\n",
       " 'william': 162781,\n",
       " 'brennan': 46717,\n",
       " 'thurgood': 150482,\n",
       " 'marshall': 104706,\n",
       " 'john': 92655,\n",
       " 'paul': 121432,\n",
       " 'byron': 48126,\n",
       " 'white': 162460,\n",
       " 'plurality': 124137,\n",
       " 'supreme': 146618,\n",
       " 'court': 56882,\n",
       " 'decision': 60337,\n",
       " 'prevented': 125851,\n",
       " 'removal': 132381,\n",
       " 'anti': 38398,\n",
       " 'american': 37546,\n",
       " 'christian': 52730,\n",
       " 'semitic': 139036,\n",
       " 'just': 93412,\n",
       " 'plain': 123805,\n",
       " 'filthy': 72926,\n",
       " 'books': 45991,\n",
       " 'public': 127119,\n",
       " 'high': 83157,\n",
       " 'school': 137970,\n",
       " 'library': 99583,\n",
       " '_board': 31966,\n",
       " 'education': 66684,\n",
       " 'pico_': 123256,\n",
       " '1982': 8404,\n",
       " 'longer': 100780,\n",
       " 'defend': 60590,\n",
       " 'free': 74740,\n",
       " 'expression': 70548,\n",
       " 'privacy': 126024,\n",
       " 'represent': 132656,\n",
       " 'boyle': 46343,\n",
       " 'cactus': 49850,\n",
       " 'org': 118738,\n",
       " 'craig': 57197,\n",
       " 'lh': 99383,\n",
       " 'workmanship': 163680,\n",
       " '1993apr15': 8471,\n",
       " '203750': 11107,\n",
       " '25764': 12923,\n",
       " 'walter': 161252,\n",
       " 'bellcore': 43776,\n",
       " 'jchen': 91578,\n",
       " 'ctt': 58083,\n",
       " 'visited': 159320,\n",
       " 'ny': 116335,\n",
       " 'auto': 40971,\n",
       " 'show': 140327,\n",
       " 'saw': 137486,\n",
       " 'cars': 50668,\n",
       " 'floor': 73603,\n",
       " 'eagle': 66119,\n",
       " 'vision': 159310,\n",
       " 'dodge': 63891,\n",
       " 'intrepid': 89395,\n",
       " 'nice': 114539,\n",
       " 'very': 158756,\n",
       " 'attractive': 40683,\n",
       " 'styling': 145662,\n",
       " 'lots': 100957,\n",
       " 'features': 72128,\n",
       " 'room': 134948,\n",
       " 'competitive': 55040,\n",
       " 'price': 125887,\n",
       " 'unfortunately': 155589,\n",
       " 'quite': 129278,\n",
       " 'disappointing': 62678,\n",
       " 'both': 46191,\n",
       " 'rubber': 135590,\n",
       " 'seals': 138584,\n",
       " 'around': 39659,\n",
       " 'window': 162881,\n",
       " 'door': 64129,\n",
       " 'fell': 72280,\n",
       " 'turns': 153249,\n",
       " 'grooved': 79533,\n",
       " 'band': 42561,\n",
       " 'by': 48075,\n",
       " 'pressing': 125766,\n",
       " 'groove': 79532,\n",
       " 'against': 36054,\n",
       " 'tongue': 151400,\n",
       " 'frame': 74600,\n",
       " 'surely': 146648,\n",
       " 'come': 54681,\n",
       " 'easily': 66206,\n",
       " 'lack': 97832,\n",
       " 'build': 47489,\n",
       " 'quality': 129039,\n",
       " 'thing': 150145,\n",
       " 'notced': 115555,\n",
       " 'first': 73107,\n",
       " 'months': 109816,\n",
       " 'panel': 120741,\n",
       " 'gaps': 76483,\n",
       " 'were': 162072,\n",
       " 'large': 98184,\n",
       " 'non': 115260,\n",
       " 'uniform': 155665,\n",
       " 'kind': 95573,\n",
       " 'expect': 70370,\n",
       " 'accept': 34728,\n",
       " 'mustang': 111663,\n",
       " 'chrysler': 52809,\n",
       " 'savior': 137474,\n",
       " 'drove': 64774,\n",
       " 'low': 101028,\n",
       " 'end': 68106,\n",
       " 'thought': 150290,\n",
       " 'adequate': 35370,\n",
       " 'prefer': 125539,\n",
       " 'taurus': 148664,\n",
       " 'brief': 46817,\n",
       " 'experience': 70421,\n",
       " 'am': 37410,\n",
       " 'sure': 146645,\n",
       " 'pooring': 124701,\n",
       " 'engineering': 68266,\n",
       " 'assembly': 40160,\n",
       " 'problems': 126127,\n",
       " 'still': 144959,\n",
       " 'consider': 55785,\n",
       " 'buying': 47915,\n",
       " 'only': 118156,\n",
       " 'establishes': 69315,\n",
       " 'good': 78629,\n",
       " 'track': 151932,\n",
       " 'record': 131453,\n",
       " 'jason': 91422,\n",
       " 'bm967': 45527,\n",
       " 'cleveland': 53675,\n",
       " 'freenet': 74769,\n",
       " 'david': 59801,\n",
       " 'kantrowitz': 94342,\n",
       " 'centris': 51545,\n",
       " '610': 22211,\n",
       " 'video': 159017,\n",
       " 'push': 127427,\n",
       " 'media': 106233,\n",
       " 'mit': 108469,\n",
       " 'pushpinder': 127438,\n",
       " 'singh': 140998,\n",
       " 'sat': 137345,\n",
       " '17': 7157,\n",
       " '1993': 8436,\n",
       " '03': 991,\n",
       " '45': 18317,\n",
       " 'computer': 55224,\n",
       " 'set': 139369,\n",
       " '256': 12886,\n",
       " 'colors': 54574,\n",
       " 'certain': 51608,\n",
       " 'operations': 118370,\n",
       " 'done': 64061,\n",
       " 'particularly': 121146,\n",
       " 'vertical': 158741,\n",
       " 'scrolling': 138345,\n",
       " 'through': 150385,\n",
       " 'horizontal': 84200,\n",
       " 'appear': 38841,\n",
       " 'monitor': 109706,\n",
       " 'generally': 77108,\n",
       " 'always': 37396,\n",
       " 'spare': 143042,\n",
       " 'open': 118316,\n",
       " 'windows': 162890,\n",
       " 'these': 150062,\n",
       " 'accummulate': 34871,\n",
       " 'operation': 118365,\n",
       " 'continued': 56066,\n",
       " 'moved': 110277,\n",
       " 'over': 119473,\n",
       " 'involved': 89565,\n",
       " 'area': 39369,\n",
       " 'screen': 138273,\n",
       " 'away': 41308,\n",
       " 'line': 99917,\n",
       " 'disappear': 62669,\n",
       " 'observed': 117070,\n",
       " 'configured': 55514,\n",
       " '16': 6577,\n",
       " '14': 5483,\n",
       " 'inch': 87741,\n",
       " 'apple': 38881,\n",
       " 'used': 156759,\n",
       " 'suspect': 146779,\n",
       " 'bad': 42268,\n",
       " 'ram': 130254,\n",
       " 'chip': 52502,\n",
       " 'cannot': 50292,\n",
       " 'apparent': 38829,\n",
       " 'since': 140968,\n",
       " 'gotten': 78764,\n",
       " 'worse': 163730,\n",
       " 'anyone': 38562,\n",
       " 'had': 81002,\n",
       " 'given': 77881,\n",
       " 'configurations': 55511,\n",
       " 'help': 82522,\n",
       " 'eliminate': 67483,\n",
       " 'design': 61472,\n",
       " 'flaw': 73442,\n",
       " 'explanation': 70466,\n",
       " 'stolk': 145087,\n",
       " 'fwi': 75556,\n",
       " 'uva': 157110,\n",
       " 'nl': 114978,\n",
       " 'bram': 46542,\n",
       " 'creating': 57327,\n",
       " 'bit': 44890,\n",
       " '24': 12449,\n",
       " 'display': 63092,\n",
       " 'greetings': 79323,\n",
       " 'using': 156812,\n",
       " 'server': 139321,\n",
       " 'provides': 126733,\n",
       " 'visuals': 159357,\n",
       " 'pseudocolor': 126873,\n",
       " 'truecolor': 152731,\n",
       " 'directcolor': 62592,\n",
       " 'occurs': 117202,\n",
       " 'try': 152804,\n",
       " 'create': 57316,\n",
       " 'visual': 159338,\n",
       " 'different': 62312,\n",
       " 'parent': 120983,\n",
       " 'uses': 156793,\n",
       " 'default': 60553,\n",
       " 'tc24': 148761,\n",
       " 'xlib': 165855,\n",
       " 'reference': 131714,\n",
       " 'guide': 79985,\n",
       " 'reilly': 132009,\n",
       " 'read': 131005,\n",
       " 'section': 138731,\n",
       " 'xcteatewindow': 165337,\n",
       " 'something': 142613,\n",
       " 'current': 58309,\n",
       " 'implementation': 87443,\n",
       " 'x11': 164527,\n",
       " 'find': 72967,\n",
       " 'suitable': 146140,\n",
       " 'colourmap': 54592,\n",
       " 'attributes': 40692,\n",
       " 'badmatch': 42299,\n",
       " 'warning': 161376,\n",
       " 'strangely': 145257,\n",
       " 'enough': 68367,\n",
       " 'mentioned': 106646,\n",
       " 'newer': 114170,\n",
       " 'editions': 66624,\n",
       " 'x11r5': 164542,\n",
       " 'guides': 79991,\n",
       " 'however': 84438,\n",
       " 'pass': 121219,\n",
       " 'along': 37238,\n",
       " 'code': 54239,\n",
       " 'here': 82682,\n",
       " 'll': 100368,\n",
       " 'destroy': 61595,\n",
       " 'crt': 57672,\n",
       " 'create_8bit_window_on_truecolour_display': 57317,\n",
       " 'dpy': 64460,\n",
       " 'width': 162649,\n",
       " 'height': 82397,\n",
       " 'int': 88959,\n",
       " 'win': 162840,\n",
       " 'xvisualinfo': 166913,\n",
       " 'vinfo': 159197,\n",
       " 'xsetwindowattributes': 166389,\n",
       " 'attr': 40672,\n",
       " 'fprintf': 74494,\n",
       " 'stderr': 144628,\n",
       " 'opening': 118337,\n",
       " 'xmatchvisualinfo': 165925,\n",
       " 'defaultscreen': 60563,\n",
       " 'handle': 81401,\n",
       " 'exit': 70300,\n",
       " 'visualid': 159341,\n",
       " 'cmap': 53979,\n",
       " 'xcreatecolormap': 165326,\n",
       " 'defaultrootwindow': 60561,\n",
       " 'allocnone': 37150,\n",
       " 'xsync': 166454,\n",
       " 'false': 71570,\n",
       " 'xinstallcolormap': 165728,\n",
       " 'ommision': 118030,\n",
       " 'gives': 77883,\n",
       " 'same': 137065,\n",
       " 'result': 133096,\n",
       " 'colormap': 54568,\n",
       " 'xcreatewindow': 165333,\n",
       " 'copyfromparent': 56414,\n",
       " 'border': 46071,\n",
       " 'depth': 61302,\n",
       " 'inputoutput': 88680,\n",
       " 'class': 53524,\n",
       " 'cwcolormap': 58497,\n",
       " 'return': 133242,\n",
       " 'executing': 70207,\n",
       " 'piece': 123285,\n",
       " 'results': 133100,\n",
       " 'error': 69068,\n",
       " 'anybody': 38547,\n",
       " 'knows': 96234,\n",
       " 'please': 123964,\n",
       " 'drop': 64758,\n",
       " '35002_4401': 16041,\n",
       " 'uwovax': 157190,\n",
       " 'uwo': 157188,\n",
       " 'ca': 49775,\n",
       " 'loopback': 100849,\n",
       " 'connector': 55683,\n",
       " 'made': 103652,\n",
       " 'need': 113686,\n",
       " 'know': 96213,\n",
       " 'pins': 123472,\n",
       " 'connect': 55668,\n",
       " 'serial': 139275,\n",
       " 'port': 124799,\n",
       " 'test': 149528,\n",
       " 'steve': 144851,\n",
       " 'csugrad': 58003,\n",
       " 'vt': 160104,\n",
       " 'kevin': 95058,\n",
       " 'thoughts': 150297,\n",
       " 'bissda': 44882,\n",
       " 'saturn': 137408,\n",
       " 'wwc': 164314,\n",
       " 'dan': 59471,\n",
       " 'lawrence': 98432,\n",
       " 'bissell': 44884,\n",
       " 'reasons': 131155,\n",
       " 'wouldn': 163770,\n",
       " 'liar': 99499,\n",
       " 'die': 62240,\n",
       " 'lie': 99668,\n",
       " 'able': 34439,\n",
       " 'tell': 149211,\n",
       " 'gathered': 76650,\n",
       " 'kept': 94986,\n",
       " 'doing': 63954,\n",
       " 'hearing': 82238,\n",
       " 'seeing': 138800,\n",
       " 'someone': 142598,\n",
       " 'healed': 82211,\n",
       " 'fool': 74028,\n",
       " 'believe': 43746,\n",
       " 'did': 62222,\n",
       " 'heal': 82210,\n",
       " 'dies': 62264,\n",
       " 'cause': 50961,\n",
       " 'risk': 133989,\n",
       " 'dying': 65586,\n",
       " 'being': 43677,\n",
       " 've': 158368,\n",
       " 'grifters': 79431,\n",
       " 'charlatans': 52085,\n",
       " 'beginning': 43609,\n",
       " 'civilization': 53260,\n",
       " 'copperfield': 56391,\n",
       " 'messiah': 106819,\n",
       " 'bet': 44126,\n",
       " 'found': 74399,\n",
       " 'plenty': 123996,\n",
       " 'believers': 43752,\n",
       " 'jesus': 91895,\n",
       " 'hardly': 81593,\n",
       " 'claim': 53451,\n",
       " 'faith': 71506,\n",
       " 'healer': 82212,\n",
       " 'wasn': 161441,\n",
       " 'witnessed': 163169,\n",
       " 'sets': 139395,\n",
       " 'apart': 38674,\n",
       " 'niether': 114636,\n",
       " 'lunatic': 101572,\n",
       " 'entire': 68469,\n",
       " 'nation': 113247,\n",
       " 'drawn': 64601,\n",
       " 'crazy': 57285,\n",
       " 'doubtful': 64261,\n",
       " 'fact': 71370,\n",
       " 'rediculous': 131572,\n",
       " 'example': 70059,\n",
       " 'koresh': 96491,\n",
       " 'obviously': 117128,\n",
       " 'logical': 100698,\n",
       " 'right': 133832,\n",
       " 'rubbish': 135595,\n",
       " 'nations': 113269,\n",
       " 'followed': 73965,\n",
       " 'crazies': 57281,\n",
       " 'liars': 99500,\n",
       " 'psychopaths': 127001,\n",
       " 'megalomaniacs': 106358,\n",
       " 'throughout': 150387,\n",
       " 'history': 83401,\n",
       " 'hitler': 83422,\n",
       " 'tojo': 151280,\n",
       " 'mussolini': 111657,\n",
       " 'khomeini': 95364,\n",
       " 'qadaffi': 128144,\n",
       " 'stalin': 144309,\n",
       " 'papa': 120822,\n",
       " 'doc': 63823,\n",
       " 'nixon': 114853,\n",
       " 'mind': 108033,\n",
       " 'century': 51557,\n",
       " 'issue': 90271,\n",
       " 'therefore': 150009,\n",
       " 'real': 131047,\n",
       " 'discrete': 62843,\n",
       " 'mathematics': 104977,\n",
       " 'formal': 74233,\n",
       " 'logic': 100695,\n",
       " 'course': 56877,\n",
       " 'flaws': 73446,\n",
       " 'everywhere': 69828,\n",
       " 'others': 119150,\n",
       " 'faq': 71671,\n",
       " 'things': 150151,\n",
       " 'fulfilled': 75281,\n",
       " 'loads': 100560,\n",
       " 'prophecies': 126509,\n",
       " 'psalms': 126852,\n",
       " 'isaiah': 90048,\n",
       " 'elsewhere': 67613,\n",
       " 'hrs': 84721,\n",
       " 'alone': 37237,\n",
       " 'his': 83362,\n",
       " 'betrayal': 44152,\n",
       " 'crucifixion': 57689,\n",
       " 'bible': 44477,\n",
       " 'moment': 109625,\n",
       " 'use': 156752,\n",
       " 'because': 43458,\n",
       " 'written': 163986,\n",
       " 'age': 36075,\n",
       " 'tome': 151357,\n",
       " 'external': 70609,\n",
       " 'supporting': 146577,\n",
       " 'evidence': 69841,\n",
       " 'makes': 104012,\n",
       " 'less': 99138,\n",
       " 'credible': 57348,\n",
       " 'quote': 129309,\n",
       " 'future': 75481,\n",
       " 'otherwise': 119153,\n",
       " 'flamed': 73360,\n",
       " 'mercilessly': 106701,\n",
       " 'most': 110097,\n",
       " 'certainly': 51612,\n",
       " 'churches': 52866,\n",
       " 'rather': 130509,\n",
       " 'should': 140299,\n",
       " 'way': 161577,\n",
       " 'total': 151638,\n",
       " 'sacrafice': 136758,\n",
       " 'everything': 69825,\n",
       " 'god': 78440,\n",
       " 'sake': 136931,\n",
       " 'loved': 101010,\n",
       " 'us': 156710,\n",
       " 'save': 137457,\n",
       " 'hey': 82877,\n",
       " 'himself': 83264,\n",
       " 'inspires': 88806,\n",
       " 'turn': 153232,\n",
       " 'our': 119250,\n",
       " 'lives': 100209,\n",
       " 'tuff': 153070,\n",
       " 'strong': 145494,\n",
       " 'persevere': 122378,\n",
       " 'weight': 161942,\n",
       " 'lifting': 99725,\n",
       " 'guitar': 80019,\n",
       " 'playing': 123934,\n",
       " 'drums': 64817,\n",
       " 'whatever': 162307,\n",
       " 'takes': 148299,\n",
       " 'rush': 135797,\n",
       " 'christianity': 52738,\n",
       " 'whole': 162529,\n",
       " 'church': 52864,\n",
       " 'once': 118095,\n",
       " 'helping': 82537,\n",
       " 'poor': 124696,\n",
       " 'while': 162402,\n",
       " 'box': 46309,\n",
       " 'units': 155772,\n",
       " 'sports': 143564,\n",
       " 'tv': 153306,\n",
       " 'above': 34493,\n",
       " 'boxes': 46313,\n",
       " 'carried': 50652,\n",
       " 'created': 57320,\n",
       " 'ourselves': 119254,\n",
       " 'eh': 66983,\n",
       " 'define': 60644,\n",
       " 'world': 163701,\n",
       " 'imagine': 87160,\n",
       " 'sp': 142910,\n",
       " 'involve': 89564,\n",
       " 'commitment': 54825,\n",
       " 'correct': 56567,\n",
       " 'tendency': 149307,\n",
       " 'explain': 70457,\n",
       " 'involove': 89560,\n",
       " 'hehehe': 82385,\n",
       " 'television': 149202,\n",
       " '__': 31615,\n",
       " '_______': 31634,\n",
       " 'science': 138064,\n",
       " 'department': 61192,\n",
       " 'virginia': 159248,\n",
       " 'tech': 148955,\n",
       " 'blacksburg': 45132,\n",
       " '703': 24464,\n",
       " '232': 12212,\n",
       " '6529': 22702,\n",
       " 'terziogl': 149510,\n",
       " 'ee': 66706,\n",
       " 'rochester': 134706,\n",
       " 'esin': 69209,\n",
       " 'terzioglu': 149511,\n",
       " 'armenia': 39567,\n",
       " 'says': 137521,\n",
       " 'shoot': 140221,\n",
       " 'turkish': 153216,\n",
       " 'planes': 123825,\n",
       " '1993apr19': 8475,\n",
       " '155856': 6405,\n",
       " '8260': 27289,\n",
       " 'kpc': 96622,\n",
       " 'henrik': 82627,\n",
       " 'quayle': 129125,\n",
       " '1993apr17': 8473,\n",
       " '185118': 7853,\n",
       " '10792': 3799,\n",
       " '1993apr16': 8472,\n",
       " '195452': 8311,\n",
       " '21375': 11560,\n",
       " 'urartu': 156612,\n",
       " 'sdpa': 138509,\n",
       " 'dbd': 59927,\n",
       " 'davidian': 59816,\n",
       " '04': 1163,\n",
       " '1045': 3674,\n",
       " 'ermenistan': 69013,\n",
       " 'kasiniyor': 94483,\n",
       " 'translate': 152173,\n",
       " 'everyone': 69820,\n",
       " 'before': 43580,\n",
       " 'traslation': 152283,\n",
       " 'service': 139334,\n",
       " 'gets': 77428,\n",
       " 'getting': 77433,\n",
       " 'itchy': 90338,\n",
       " 'clearify': 53623,\n",
       " 'mr': 110614,\n",
       " 'she': 139885,\n",
       " 'simply': 140908,\n",
       " 'letting': 99184,\n",
       " 'sit': 141105,\n",
       " 'quiet': 129223,\n",
       " 'turks': 153227,\n",
       " 'their': 149895,\n",
       " 'famous': 71622,\n",
       " 'tricks': 152485,\n",
       " 'armenians': 39570,\n",
       " 'remember': 132326,\n",
       " 'invasion': 89481,\n",
       " 'greek': 79281,\n",
       " 'island': 90144,\n",
       " 'cypress': 58759,\n",
       " 'watched': 161469,\n",
       " 'ignorance': 86700,\n",
       " 'obvious': 117126,\n",
       " 'posting': 124953,\n",
       " 'cyprus': 58764,\n",
       " 'independent': 87960,\n",
       " 'country': 56837,\n",
       " 'inhabitants': 88454,\n",
       " 'ignorant': 86701,\n",
       " 'claims': 53457,\n",
       " 'name': 113066,\n",
       " 'english': 68277,\n",
       " 'learn': 98735,\n",
       " 'post': 124921,\n",
       " 'rjh': 134110,\n",
       " 'allegra': 37091,\n",
       " 'att': 40585,\n",
       " 'robert': 134633,\n",
       " 'holt': 83949,\n",
       " 'best': 44116,\n",
       " 'players': 123926,\n",
       " '162313': 6725,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm tạo các gói(batch) để làm đầu vào(input) khi training cũng như khi test\n",
    "def get_batch(df,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df.data[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df.target[i*batch_size:i*batch_size+batch_size]\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "        processed_text = text.lower()     \n",
    "        processed_text = re.sub('[^a-z]+', \" \", processed_text)\n",
    "        \n",
    "        for word in processed_text.split(' '):\n",
    "            if word in wordindex:\n",
    "                layer[wordindex[word]] += 1     # Có thể coi layer ở đây là một feature vector của văn bản với số chiều là tổng\n",
    "                                                # số từ trong từ điển. Mỗi phần tử trong layer đại diện cho số từ tương ứng  \n",
    "                                                # xuất hiện trong văn bản đó.\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        index_y = -1\n",
    "        for j in range(20):\n",
    "            if category == j:\n",
    "                index_y = j\n",
    "                break\n",
    "    \n",
    "        results.append(index_y)\n",
    "\n",
    "\n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các tham số\n",
    "learning_rate = 0.01\n",
    "num_epochs = 15        #Số lần học\n",
    "batch_size = 150       #Kích thước các gói \n",
    "display_step = 1\n",
    "\n",
    "# Tham số của mạng(network)\n",
    "hidden_size = 100        # Kích thước của các tầng trong mạng\n",
    "input_size = total_words # Tổng số lượng các từ\n",
    "num_classes = 20         # Tổng số nhãn lớp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNet(nn.Module):\n",
    "     def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(OurNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "     def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OurNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [4/75], Loss: 2.8683\n",
      "Epoch [1/15], Step [8/75], Loss: 2.4600\n",
      "Epoch [1/15], Step [12/75], Loss: 1.8625\n",
      "Epoch [1/15], Step [16/75], Loss: 1.6037\n",
      "Epoch [1/15], Step [20/75], Loss: 1.1232\n",
      "Epoch [1/15], Step [24/75], Loss: 1.0060\n",
      "Epoch [1/15], Step [28/75], Loss: 0.8692\n",
      "Epoch [1/15], Step [32/75], Loss: 1.4223\n",
      "Epoch [1/15], Step [36/75], Loss: 0.6691\n",
      "Epoch [1/15], Step [40/75], Loss: 0.6539\n",
      "Epoch [1/15], Step [44/75], Loss: 0.9981\n",
      "Epoch [1/15], Step [48/75], Loss: 0.6884\n",
      "Epoch [1/15], Step [52/75], Loss: 0.6477\n",
      "Epoch [1/15], Step [56/75], Loss: 0.4220\n",
      "Epoch [1/15], Step [60/75], Loss: 0.5173\n",
      "Epoch [1/15], Step [64/75], Loss: 0.4559\n",
      "Epoch [1/15], Step [68/75], Loss: 0.4547\n",
      "Epoch [1/15], Step [72/75], Loss: 0.3878\n",
      "Epoch [1/15], Step [76/75], Loss: 0.4485\n",
      "Epoch [2/15], Step [4/75], Loss: 0.4602\n",
      "Epoch [2/15], Step [8/75], Loss: 0.1593\n",
      "Epoch [2/15], Step [12/75], Loss: 0.1479\n",
      "Epoch [2/15], Step [16/75], Loss: 0.0796\n",
      "Epoch [2/15], Step [20/75], Loss: 0.1462\n",
      "Epoch [2/15], Step [24/75], Loss: 0.1713\n",
      "Epoch [2/15], Step [28/75], Loss: 0.1466\n",
      "Epoch [2/15], Step [32/75], Loss: 0.0834\n",
      "Epoch [2/15], Step [36/75], Loss: 0.0751\n",
      "Epoch [2/15], Step [40/75], Loss: 0.1697\n",
      "Epoch [2/15], Step [44/75], Loss: 0.1558\n",
      "Epoch [2/15], Step [48/75], Loss: 0.2041\n",
      "Epoch [2/15], Step [52/75], Loss: 0.2819\n",
      "Epoch [2/15], Step [56/75], Loss: 0.0984\n",
      "Epoch [2/15], Step [60/75], Loss: 0.0729\n",
      "Epoch [2/15], Step [64/75], Loss: 0.1272\n",
      "Epoch [2/15], Step [68/75], Loss: 0.0842\n",
      "Epoch [2/15], Step [72/75], Loss: 0.1271\n",
      "Epoch [2/15], Step [76/75], Loss: 0.0158\n",
      "Epoch [3/15], Step [4/75], Loss: 0.0395\n",
      "Epoch [3/15], Step [8/75], Loss: 0.0051\n",
      "Epoch [3/15], Step [12/75], Loss: 0.0098\n",
      "Epoch [3/15], Step [16/75], Loss: 0.0130\n",
      "Epoch [3/15], Step [20/75], Loss: 0.0383\n",
      "Epoch [3/15], Step [24/75], Loss: 0.0781\n",
      "Epoch [3/15], Step [28/75], Loss: 0.0889\n",
      "Epoch [3/15], Step [32/75], Loss: 0.0091\n",
      "Epoch [3/15], Step [36/75], Loss: 0.0260\n",
      "Epoch [3/15], Step [40/75], Loss: 0.0172\n",
      "Epoch [3/15], Step [44/75], Loss: 0.1793\n",
      "Epoch [3/15], Step [48/75], Loss: 0.0154\n",
      "Epoch [3/15], Step [52/75], Loss: 0.1671\n",
      "Epoch [3/15], Step [56/75], Loss: 0.2772\n",
      "Epoch [3/15], Step [60/75], Loss: 0.3398\n",
      "Epoch [3/15], Step [64/75], Loss: 0.2657\n",
      "Epoch [3/15], Step [68/75], Loss: 0.1926\n",
      "Epoch [3/15], Step [72/75], Loss: 0.6336\n",
      "Epoch [3/15], Step [76/75], Loss: 0.0694\n",
      "Epoch [4/15], Step [4/75], Loss: 0.0432\n",
      "Epoch [4/15], Step [8/75], Loss: 0.5344\n",
      "Epoch [4/15], Step [12/75], Loss: 0.1097\n",
      "Epoch [4/15], Step [16/75], Loss: 0.1176\n",
      "Epoch [4/15], Step [20/75], Loss: 0.1556\n",
      "Epoch [4/15], Step [24/75], Loss: 0.2344\n",
      "Epoch [4/15], Step [28/75], Loss: 0.0593\n",
      "Epoch [4/15], Step [32/75], Loss: 0.0491\n",
      "Epoch [4/15], Step [36/75], Loss: 0.1299\n",
      "Epoch [4/15], Step [40/75], Loss: 0.0155\n",
      "Epoch [4/15], Step [44/75], Loss: 0.0312\n",
      "Epoch [4/15], Step [48/75], Loss: 0.0452\n",
      "Epoch [4/15], Step [52/75], Loss: 0.0877\n",
      "Epoch [4/15], Step [56/75], Loss: 0.0290\n",
      "Epoch [4/15], Step [60/75], Loss: 0.3620\n",
      "Epoch [4/15], Step [64/75], Loss: 0.0288\n",
      "Epoch [4/15], Step [68/75], Loss: 0.0318\n",
      "Epoch [4/15], Step [72/75], Loss: 0.0080\n",
      "Epoch [4/15], Step [76/75], Loss: 0.0015\n",
      "Epoch [5/15], Step [4/75], Loss: 0.0945\n",
      "Epoch [5/15], Step [8/75], Loss: 0.0013\n",
      "Epoch [5/15], Step [12/75], Loss: 0.0137\n",
      "Epoch [5/15], Step [16/75], Loss: 0.1443\n",
      "Epoch [5/15], Step [20/75], Loss: 0.1167\n",
      "Epoch [5/15], Step [24/75], Loss: 0.2170\n",
      "Epoch [5/15], Step [28/75], Loss: 0.0046\n",
      "Epoch [5/15], Step [32/75], Loss: 0.0091\n",
      "Epoch [5/15], Step [36/75], Loss: 0.0400\n",
      "Epoch [5/15], Step [40/75], Loss: 0.1543\n",
      "Epoch [5/15], Step [44/75], Loss: 0.6117\n",
      "Epoch [5/15], Step [48/75], Loss: 0.2044\n",
      "Epoch [5/15], Step [52/75], Loss: 0.0853\n",
      "Epoch [5/15], Step [56/75], Loss: 0.0373\n",
      "Epoch [5/15], Step [60/75], Loss: 0.1203\n",
      "Epoch [5/15], Step [64/75], Loss: 0.0057\n",
      "Epoch [5/15], Step [68/75], Loss: 0.0354\n",
      "Epoch [5/15], Step [72/75], Loss: 0.0199\n",
      "Epoch [5/15], Step [76/75], Loss: 0.0007\n",
      "Epoch [6/15], Step [4/75], Loss: 0.0064\n",
      "Epoch [6/15], Step [8/75], Loss: 0.0046\n",
      "Epoch [6/15], Step [12/75], Loss: 0.0120\n",
      "Epoch [6/15], Step [16/75], Loss: 0.1094\n",
      "Epoch [6/15], Step [20/75], Loss: 0.0225\n",
      "Epoch [6/15], Step [24/75], Loss: 0.1363\n",
      "Epoch [6/15], Step [28/75], Loss: 0.0093\n",
      "Epoch [6/15], Step [32/75], Loss: 0.0006\n",
      "Epoch [6/15], Step [36/75], Loss: 0.0106\n",
      "Epoch [6/15], Step [40/75], Loss: 0.0019\n",
      "Epoch [6/15], Step [44/75], Loss: 0.4606\n",
      "Epoch [6/15], Step [48/75], Loss: 0.0011\n",
      "Epoch [6/15], Step [52/75], Loss: 0.0123\n",
      "Epoch [6/15], Step [56/75], Loss: 0.2045\n",
      "Epoch [6/15], Step [60/75], Loss: 0.0002\n",
      "Epoch [6/15], Step [64/75], Loss: 0.0001\n",
      "Epoch [6/15], Step [68/75], Loss: 0.0021\n",
      "Epoch [6/15], Step [72/75], Loss: 0.0007\n",
      "Epoch [6/15], Step [76/75], Loss: 0.0001\n",
      "Epoch [7/15], Step [4/75], Loss: 0.0138\n",
      "Epoch [7/15], Step [8/75], Loss: 0.0017\n",
      "Epoch [7/15], Step [12/75], Loss: 0.0006\n",
      "Epoch [7/15], Step [16/75], Loss: 0.0011\n",
      "Epoch [7/15], Step [20/75], Loss: 0.0498\n",
      "Epoch [7/15], Step [24/75], Loss: 0.1227\n",
      "Epoch [7/15], Step [28/75], Loss: 0.0022\n",
      "Epoch [7/15], Step [32/75], Loss: 0.0257\n",
      "Epoch [7/15], Step [36/75], Loss: 0.0206\n",
      "Epoch [7/15], Step [40/75], Loss: 0.0022\n",
      "Epoch [7/15], Step [44/75], Loss: 0.0005\n",
      "Epoch [7/15], Step [48/75], Loss: 0.0006\n",
      "Epoch [7/15], Step [52/75], Loss: 0.0544\n",
      "Epoch [7/15], Step [56/75], Loss: 0.6570\n",
      "Epoch [7/15], Step [60/75], Loss: 0.0039\n",
      "Epoch [7/15], Step [64/75], Loss: 0.0110\n",
      "Epoch [7/15], Step [68/75], Loss: 0.7218\n",
      "Epoch [7/15], Step [72/75], Loss: 0.0097\n",
      "Epoch [7/15], Step [76/75], Loss: 0.0054\n",
      "Epoch [8/15], Step [4/75], Loss: 0.0103\n",
      "Epoch [8/15], Step [8/75], Loss: 0.0172\n",
      "Epoch [8/15], Step [12/75], Loss: 0.0037\n",
      "Epoch [8/15], Step [16/75], Loss: 0.0626\n",
      "Epoch [8/15], Step [20/75], Loss: 0.1115\n",
      "Epoch [8/15], Step [24/75], Loss: 0.0869\n",
      "Epoch [8/15], Step [28/75], Loss: 0.0053\n",
      "Epoch [8/15], Step [32/75], Loss: 0.0025\n",
      "Epoch [8/15], Step [36/75], Loss: 0.1292\n",
      "Epoch [8/15], Step [40/75], Loss: 0.0007\n",
      "Epoch [8/15], Step [44/75], Loss: 0.0023\n",
      "Epoch [8/15], Step [48/75], Loss: 0.0656\n",
      "Epoch [8/15], Step [52/75], Loss: 0.0218\n",
      "Epoch [8/15], Step [56/75], Loss: 0.0097\n",
      "Epoch [8/15], Step [60/75], Loss: 0.0104\n",
      "Epoch [8/15], Step [64/75], Loss: 0.0009\n",
      "Epoch [8/15], Step [68/75], Loss: 0.2091\n",
      "Epoch [8/15], Step [72/75], Loss: 0.0006\n",
      "Epoch [8/15], Step [76/75], Loss: 0.0000\n",
      "Epoch [9/15], Step [4/75], Loss: 0.0158\n",
      "Epoch [9/15], Step [8/75], Loss: 0.0170\n",
      "Epoch [9/15], Step [12/75], Loss: 0.0005\n",
      "Epoch [9/15], Step [16/75], Loss: 0.0008\n",
      "Epoch [9/15], Step [20/75], Loss: 0.0623\n",
      "Epoch [9/15], Step [24/75], Loss: 0.1129\n",
      "Epoch [9/15], Step [28/75], Loss: 0.0156\n",
      "Epoch [9/15], Step [32/75], Loss: 0.0002\n",
      "Epoch [9/15], Step [36/75], Loss: 0.0027\n",
      "Epoch [9/15], Step [40/75], Loss: 0.0024\n",
      "Epoch [9/15], Step [44/75], Loss: 0.0005\n",
      "Epoch [9/15], Step [48/75], Loss: 0.0004\n",
      "Epoch [9/15], Step [52/75], Loss: 0.0696\n",
      "Epoch [9/15], Step [56/75], Loss: 0.0116\n",
      "Epoch [9/15], Step [60/75], Loss: 0.0612\n",
      "Epoch [9/15], Step [64/75], Loss: 0.0003\n",
      "Epoch [9/15], Step [68/75], Loss: 0.0081\n",
      "Epoch [9/15], Step [72/75], Loss: 0.0251\n",
      "Epoch [9/15], Step [76/75], Loss: 0.0038\n",
      "Epoch [10/15], Step [4/75], Loss: 0.0038\n",
      "Epoch [10/15], Step [8/75], Loss: 0.0000\n",
      "Epoch [10/15], Step [12/75], Loss: 0.0015\n",
      "Epoch [10/15], Step [16/75], Loss: 0.0001\n",
      "Epoch [10/15], Step [20/75], Loss: 0.0081\n",
      "Epoch [10/15], Step [24/75], Loss: 0.1494\n",
      "Epoch [10/15], Step [28/75], Loss: 0.0010\n",
      "Epoch [10/15], Step [32/75], Loss: 0.0039\n",
      "Epoch [10/15], Step [36/75], Loss: 0.0053\n",
      "Epoch [10/15], Step [40/75], Loss: 0.0232\n",
      "Epoch [10/15], Step [44/75], Loss: 0.0002\n",
      "Epoch [10/15], Step [48/75], Loss: 0.0002\n",
      "Epoch [10/15], Step [52/75], Loss: 0.0141\n",
      "Epoch [10/15], Step [56/75], Loss: 0.0024\n",
      "Epoch [10/15], Step [60/75], Loss: 0.0001\n",
      "Epoch [10/15], Step [64/75], Loss: 0.0001\n",
      "Epoch [10/15], Step [68/75], Loss: 0.0002\n",
      "Epoch [10/15], Step [72/75], Loss: 0.0004\n",
      "Epoch [10/15], Step [76/75], Loss: 0.0002\n",
      "Epoch [11/15], Step [4/75], Loss: 0.0007\n",
      "Epoch [11/15], Step [8/75], Loss: 0.0003\n",
      "Epoch [11/15], Step [12/75], Loss: 0.0041\n",
      "Epoch [11/15], Step [16/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [20/75], Loss: 0.0008\n",
      "Epoch [11/15], Step [24/75], Loss: 0.0748\n",
      "Epoch [11/15], Step [28/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [32/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [36/75], Loss: 0.0002\n",
      "Epoch [11/15], Step [40/75], Loss: 0.0002\n",
      "Epoch [11/15], Step [44/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [48/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [52/75], Loss: 0.0405\n",
      "Epoch [11/15], Step [56/75], Loss: 0.0003\n",
      "Epoch [11/15], Step [60/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [64/75], Loss: 0.0332\n",
      "Epoch [11/15], Step [68/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [72/75], Loss: 0.0001\n",
      "Epoch [11/15], Step [76/75], Loss: 0.0000\n",
      "Epoch [12/15], Step [4/75], Loss: 0.0079\n",
      "Epoch [12/15], Step [8/75], Loss: 0.0000\n",
      "Epoch [12/15], Step [12/75], Loss: 0.0000\n",
      "Epoch [12/15], Step [16/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [20/75], Loss: 0.0006\n",
      "Epoch [12/15], Step [24/75], Loss: 0.0505\n",
      "Epoch [12/15], Step [28/75], Loss: 0.0002\n",
      "Epoch [12/15], Step [32/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [36/75], Loss: 0.0002\n",
      "Epoch [12/15], Step [40/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [44/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [48/75], Loss: 0.0002\n",
      "Epoch [12/15], Step [52/75], Loss: 0.0295\n",
      "Epoch [12/15], Step [56/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [60/75], Loss: 0.0002\n",
      "Epoch [12/15], Step [64/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [68/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [72/75], Loss: 0.0001\n",
      "Epoch [12/15], Step [76/75], Loss: 0.0001\n",
      "Epoch [13/15], Step [4/75], Loss: 0.1535\n",
      "Epoch [13/15], Step [8/75], Loss: 0.0277\n",
      "Epoch [13/15], Step [12/75], Loss: 0.0015\n",
      "Epoch [13/15], Step [16/75], Loss: 0.1793\n",
      "Epoch [13/15], Step [20/75], Loss: 0.0117\n",
      "Epoch [13/15], Step [24/75], Loss: 0.1399\n",
      "Epoch [13/15], Step [28/75], Loss: 0.3715\n",
      "Epoch [13/15], Step [32/75], Loss: 0.0249\n",
      "Epoch [13/15], Step [36/75], Loss: 0.0823\n",
      "Epoch [13/15], Step [40/75], Loss: 0.0266\n",
      "Epoch [13/15], Step [44/75], Loss: 0.1155\n",
      "Epoch [13/15], Step [48/75], Loss: 0.0101\n",
      "Epoch [13/15], Step [52/75], Loss: 0.0498\n",
      "Epoch [13/15], Step [56/75], Loss: 0.7794\n",
      "Epoch [13/15], Step [60/75], Loss: 0.1372\n",
      "Epoch [13/15], Step [64/75], Loss: 0.0026\n",
      "Epoch [13/15], Step [68/75], Loss: 0.0180\n",
      "Epoch [13/15], Step [72/75], Loss: 0.0961\n",
      "Epoch [13/15], Step [76/75], Loss: 0.0002\n",
      "Epoch [14/15], Step [4/75], Loss: 0.2084\n",
      "Epoch [14/15], Step [8/75], Loss: 0.1267\n",
      "Epoch [14/15], Step [12/75], Loss: 0.1684\n",
      "Epoch [14/15], Step [16/75], Loss: 0.0390\n",
      "Epoch [14/15], Step [20/75], Loss: 0.0176\n",
      "Epoch [14/15], Step [24/75], Loss: 0.6446\n",
      "Epoch [14/15], Step [28/75], Loss: 0.0487\n",
      "Epoch [14/15], Step [32/75], Loss: 0.0073\n",
      "Epoch [14/15], Step [36/75], Loss: 0.2551\n",
      "Epoch [14/15], Step [40/75], Loss: 0.0075\n",
      "Epoch [14/15], Step [44/75], Loss: 0.0219\n",
      "Epoch [14/15], Step [48/75], Loss: 0.0201\n",
      "Epoch [14/15], Step [52/75], Loss: 0.1153\n",
      "Epoch [14/15], Step [56/75], Loss: 0.0302\n",
      "Epoch [14/15], Step [60/75], Loss: 0.0154\n",
      "Epoch [14/15], Step [64/75], Loss: 0.0021\n",
      "Epoch [14/15], Step [68/75], Loss: 0.1909\n",
      "Epoch [14/15], Step [72/75], Loss: 0.0095\n",
      "Epoch [14/15], Step [76/75], Loss: 1.3317\n",
      "Epoch [15/15], Step [4/75], Loss: 0.1003\n",
      "Epoch [15/15], Step [8/75], Loss: 0.2504\n",
      "Epoch [15/15], Step [12/75], Loss: 0.0462\n",
      "Epoch [15/15], Step [16/75], Loss: 0.2990\n",
      "Epoch [15/15], Step [20/75], Loss: 0.0491\n",
      "Epoch [15/15], Step [24/75], Loss: 0.2580\n",
      "Epoch [15/15], Step [28/75], Loss: 0.0126\n",
      "Epoch [15/15], Step [32/75], Loss: 0.0391\n",
      "Epoch [15/15], Step [36/75], Loss: 0.0033\n",
      "Epoch [15/15], Step [40/75], Loss: 0.0405\n",
      "Epoch [15/15], Step [44/75], Loss: 0.0220\n",
      "Epoch [15/15], Step [48/75], Loss: 0.0390\n",
      "Epoch [15/15], Step [52/75], Loss: 0.0749\n",
      "Epoch [15/15], Step [56/75], Loss: 0.0098\n",
      "Epoch [15/15], Step [60/75], Loss: 0.0001\n",
      "Epoch [15/15], Step [64/75], Loss: 0.0368\n",
      "Epoch [15/15], Step [68/75], Loss: 0.4709\n",
      "Epoch [15/15], Step [72/75], Loss: 0.0026\n",
      "Epoch [15/15], Step [76/75], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Training num_epochs = 15\n",
    "# Hàm Loss và Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(newsgroups_train.data)/batch_size)\n",
    "    #  Lặp lại qua từng gói\n",
    "    for i in range(total_batch + 1):\n",
    "        batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()            # Đưa các giá trị gradient về 0\n",
    "        outputs = net(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()                  # Tính toán các gradient  \n",
    "        optimizer.step()                 # Cập nhật các tham số\n",
    "\n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(newsgroups_train.data)//batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test articles: 76 %\n"
     ]
    }
   ],
   "source": [
    "# Test model num_epochs = 15\n",
    "correct = 0\n",
    "\n",
    "total_test_data = len(newsgroups_test.target)\n",
    "total_batch = int(total_test_data/batch_size)\n",
    "for i in range(total_batch + 1):\n",
    "    batch_x_test,batch_y_test = get_batch(newsgroups_test,i,batch_size)\n",
    "    articles = Variable(torch.FloatTensor(batch_x_test))\n",
    "    labels = torch.LongTensor(batch_y_test)\n",
    "    \n",
    "    outputs = net(articles)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "   \n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the network on the test articles: %d %%' % (100 * correct / total_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [4/75], Loss: 3.0007\n",
      "Epoch [1/20], Step [8/75], Loss: 2.6710\n",
      "Epoch [1/20], Step [12/75], Loss: 2.2359\n",
      "Epoch [1/20], Step [16/75], Loss: 1.7174\n",
      "Epoch [1/20], Step [20/75], Loss: 1.2788\n",
      "Epoch [1/20], Step [24/75], Loss: 2.1794\n",
      "Epoch [1/20], Step [28/75], Loss: 1.1663\n",
      "Epoch [1/20], Step [32/75], Loss: 0.9954\n",
      "Epoch [1/20], Step [36/75], Loss: 0.9206\n",
      "Epoch [1/20], Step [40/75], Loss: 0.7143\n",
      "Epoch [1/20], Step [44/75], Loss: 0.9131\n",
      "Epoch [1/20], Step [48/75], Loss: 0.7884\n",
      "Epoch [1/20], Step [52/75], Loss: 0.7827\n",
      "Epoch [1/20], Step [56/75], Loss: 0.7314\n",
      "Epoch [1/20], Step [60/75], Loss: 0.5536\n",
      "Epoch [1/20], Step [64/75], Loss: 0.4110\n",
      "Epoch [1/20], Step [68/75], Loss: 0.3580\n",
      "Epoch [1/20], Step [72/75], Loss: 0.6228\n",
      "Epoch [1/20], Step [76/75], Loss: 0.3584\n",
      "Epoch [2/20], Step [4/75], Loss: 0.3665\n",
      "Epoch [2/20], Step [8/75], Loss: 0.2262\n",
      "Epoch [2/20], Step [12/75], Loss: 0.1920\n",
      "Epoch [2/20], Step [16/75], Loss: 0.3752\n",
      "Epoch [2/20], Step [20/75], Loss: 0.1260\n",
      "Epoch [2/20], Step [24/75], Loss: 1.8266\n",
      "Epoch [2/20], Step [28/75], Loss: 0.0798\n",
      "Epoch [2/20], Step [32/75], Loss: 0.1820\n",
      "Epoch [2/20], Step [36/75], Loss: 0.1116\n",
      "Epoch [2/20], Step [40/75], Loss: 0.0856\n",
      "Epoch [2/20], Step [44/75], Loss: 0.1453\n",
      "Epoch [2/20], Step [48/75], Loss: 0.0607\n",
      "Epoch [2/20], Step [52/75], Loss: 0.0385\n",
      "Epoch [2/20], Step [56/75], Loss: 0.3482\n",
      "Epoch [2/20], Step [60/75], Loss: 0.0459\n",
      "Epoch [2/20], Step [64/75], Loss: 0.0599\n",
      "Epoch [2/20], Step [68/75], Loss: 0.0987\n",
      "Epoch [2/20], Step [72/75], Loss: 0.1618\n",
      "Epoch [2/20], Step [76/75], Loss: 0.0218\n",
      "Epoch [3/20], Step [4/75], Loss: 0.1735\n",
      "Epoch [3/20], Step [8/75], Loss: 0.0830\n",
      "Epoch [3/20], Step [12/75], Loss: 0.1401\n",
      "Epoch [3/20], Step [16/75], Loss: 0.1225\n",
      "Epoch [3/20], Step [20/75], Loss: 0.0825\n",
      "Epoch [3/20], Step [24/75], Loss: 0.0987\n",
      "Epoch [3/20], Step [28/75], Loss: 0.0845\n",
      "Epoch [3/20], Step [32/75], Loss: 0.0520\n",
      "Epoch [3/20], Step [36/75], Loss: 0.0298\n",
      "Epoch [3/20], Step [40/75], Loss: 0.0092\n",
      "Epoch [3/20], Step [44/75], Loss: 0.0659\n",
      "Epoch [3/20], Step [48/75], Loss: 0.0055\n",
      "Epoch [3/20], Step [52/75], Loss: 0.0059\n",
      "Epoch [3/20], Step [56/75], Loss: 0.3609\n",
      "Epoch [3/20], Step [60/75], Loss: 0.0410\n",
      "Epoch [3/20], Step [64/75], Loss: 0.0084\n",
      "Epoch [3/20], Step [68/75], Loss: 0.1935\n",
      "Epoch [3/20], Step [72/75], Loss: 0.0149\n",
      "Epoch [3/20], Step [76/75], Loss: 0.0094\n",
      "Epoch [4/20], Step [4/75], Loss: 0.0118\n",
      "Epoch [4/20], Step [8/75], Loss: 0.0035\n",
      "Epoch [4/20], Step [12/75], Loss: 0.2370\n",
      "Epoch [4/20], Step [16/75], Loss: 0.1087\n",
      "Epoch [4/20], Step [20/75], Loss: 0.0256\n",
      "Epoch [4/20], Step [24/75], Loss: 0.1899\n",
      "Epoch [4/20], Step [28/75], Loss: 0.0412\n",
      "Epoch [4/20], Step [32/75], Loss: 0.0342\n",
      "Epoch [4/20], Step [36/75], Loss: 0.0036\n",
      "Epoch [4/20], Step [40/75], Loss: 0.0063\n",
      "Epoch [4/20], Step [44/75], Loss: 0.0073\n",
      "Epoch [4/20], Step [48/75], Loss: 0.0066\n",
      "Epoch [4/20], Step [52/75], Loss: 0.0054\n",
      "Epoch [4/20], Step [56/75], Loss: 0.2808\n",
      "Epoch [4/20], Step [60/75], Loss: 0.0312\n",
      "Epoch [4/20], Step [64/75], Loss: 0.0244\n",
      "Epoch [4/20], Step [68/75], Loss: 0.0019\n",
      "Epoch [4/20], Step [72/75], Loss: 0.0218\n",
      "Epoch [4/20], Step [76/75], Loss: 0.0010\n",
      "Epoch [5/20], Step [4/75], Loss: 0.0212\n",
      "Epoch [5/20], Step [8/75], Loss: 0.0009\n",
      "Epoch [5/20], Step [12/75], Loss: 0.0201\n",
      "Epoch [5/20], Step [16/75], Loss: 0.0658\n",
      "Epoch [5/20], Step [20/75], Loss: 0.0184\n",
      "Epoch [5/20], Step [24/75], Loss: 0.6596\n",
      "Epoch [5/20], Step [28/75], Loss: 0.0051\n",
      "Epoch [5/20], Step [32/75], Loss: 0.0146\n",
      "Epoch [5/20], Step [36/75], Loss: 0.1624\n",
      "Epoch [5/20], Step [40/75], Loss: 0.0805\n",
      "Epoch [5/20], Step [44/75], Loss: 0.0055\n",
      "Epoch [5/20], Step [48/75], Loss: 0.0396\n",
      "Epoch [5/20], Step [52/75], Loss: 0.0310\n",
      "Epoch [5/20], Step [56/75], Loss: 0.2609\n",
      "Epoch [5/20], Step [60/75], Loss: 0.0237\n",
      "Epoch [5/20], Step [64/75], Loss: 0.0906\n",
      "Epoch [5/20], Step [68/75], Loss: 0.0497\n",
      "Epoch [5/20], Step [72/75], Loss: 0.0876\n",
      "Epoch [5/20], Step [76/75], Loss: 0.0016\n",
      "Epoch [6/20], Step [4/75], Loss: 0.0063\n",
      "Epoch [6/20], Step [8/75], Loss: 0.0274\n",
      "Epoch [6/20], Step [12/75], Loss: 0.0318\n",
      "Epoch [6/20], Step [16/75], Loss: 0.1257\n",
      "Epoch [6/20], Step [20/75], Loss: 0.0173\n",
      "Epoch [6/20], Step [24/75], Loss: 3.5203\n",
      "Epoch [6/20], Step [28/75], Loss: 0.0256\n",
      "Epoch [6/20], Step [32/75], Loss: 0.3748\n",
      "Epoch [6/20], Step [36/75], Loss: 0.1549\n",
      "Epoch [6/20], Step [40/75], Loss: 0.1006\n",
      "Epoch [6/20], Step [44/75], Loss: 0.1129\n",
      "Epoch [6/20], Step [48/75], Loss: 0.0227\n",
      "Epoch [6/20], Step [52/75], Loss: 0.0168\n",
      "Epoch [6/20], Step [56/75], Loss: 0.1308\n",
      "Epoch [6/20], Step [60/75], Loss: 0.0192\n",
      "Epoch [6/20], Step [64/75], Loss: 0.0770\n",
      "Epoch [6/20], Step [68/75], Loss: 0.3889\n",
      "Epoch [6/20], Step [72/75], Loss: 0.4278\n",
      "Epoch [6/20], Step [76/75], Loss: 0.0175\n",
      "Epoch [7/20], Step [4/75], Loss: 0.0501\n",
      "Epoch [7/20], Step [8/75], Loss: 0.3233\n",
      "Epoch [7/20], Step [12/75], Loss: 0.1648\n",
      "Epoch [7/20], Step [16/75], Loss: 0.2849\n",
      "Epoch [7/20], Step [20/75], Loss: 0.2347\n",
      "Epoch [7/20], Step [24/75], Loss: 0.9476\n",
      "Epoch [7/20], Step [28/75], Loss: 0.1164\n",
      "Epoch [7/20], Step [32/75], Loss: 0.0617\n",
      "Epoch [7/20], Step [36/75], Loss: 0.0242\n",
      "Epoch [7/20], Step [40/75], Loss: 0.0802\n",
      "Epoch [7/20], Step [44/75], Loss: 0.0114\n",
      "Epoch [7/20], Step [48/75], Loss: 0.1509\n",
      "Epoch [7/20], Step [52/75], Loss: 0.1861\n",
      "Epoch [7/20], Step [56/75], Loss: 0.0914\n",
      "Epoch [7/20], Step [60/75], Loss: 0.0040\n",
      "Epoch [7/20], Step [64/75], Loss: 0.0645\n",
      "Epoch [7/20], Step [68/75], Loss: 0.0025\n",
      "Epoch [7/20], Step [72/75], Loss: 0.2439\n",
      "Epoch [7/20], Step [76/75], Loss: 0.0005\n",
      "Epoch [8/20], Step [4/75], Loss: 0.0608\n",
      "Epoch [8/20], Step [8/75], Loss: 0.0300\n",
      "Epoch [8/20], Step [12/75], Loss: 0.0096\n",
      "Epoch [8/20], Step [16/75], Loss: 0.1618\n",
      "Epoch [8/20], Step [20/75], Loss: 0.0599\n",
      "Epoch [8/20], Step [24/75], Loss: 0.1706\n",
      "Epoch [8/20], Step [28/75], Loss: 0.2569\n",
      "Epoch [8/20], Step [32/75], Loss: 0.0497\n",
      "Epoch [8/20], Step [36/75], Loss: 0.1010\n",
      "Epoch [8/20], Step [40/75], Loss: 0.1183\n",
      "Epoch [8/20], Step [44/75], Loss: 0.3564\n",
      "Epoch [8/20], Step [48/75], Loss: 0.1539\n",
      "Epoch [8/20], Step [52/75], Loss: 0.3145\n",
      "Epoch [8/20], Step [56/75], Loss: 0.1262\n",
      "Epoch [8/20], Step [60/75], Loss: 0.0233\n",
      "Epoch [8/20], Step [64/75], Loss: 0.0031\n",
      "Epoch [8/20], Step [68/75], Loss: 0.0120\n",
      "Epoch [8/20], Step [72/75], Loss: 0.0793\n",
      "Epoch [8/20], Step [76/75], Loss: 0.1470\n",
      "Epoch [9/20], Step [4/75], Loss: 0.0174\n",
      "Epoch [9/20], Step [8/75], Loss: 0.1163\n",
      "Epoch [9/20], Step [12/75], Loss: 0.0178\n",
      "Epoch [9/20], Step [16/75], Loss: 0.2987\n",
      "Epoch [9/20], Step [20/75], Loss: 0.1419\n",
      "Epoch [9/20], Step [24/75], Loss: 0.0464\n",
      "Epoch [9/20], Step [28/75], Loss: 0.0465\n",
      "Epoch [9/20], Step [32/75], Loss: 0.0067\n",
      "Epoch [9/20], Step [36/75], Loss: 0.0133\n",
      "Epoch [9/20], Step [40/75], Loss: 0.0012\n",
      "Epoch [9/20], Step [44/75], Loss: 0.0628\n",
      "Epoch [9/20], Step [48/75], Loss: 0.0065\n",
      "Epoch [9/20], Step [52/75], Loss: 0.0217\n",
      "Epoch [9/20], Step [56/75], Loss: 0.0154\n",
      "Epoch [9/20], Step [60/75], Loss: 0.2535\n",
      "Epoch [9/20], Step [64/75], Loss: 0.0109\n",
      "Epoch [9/20], Step [68/75], Loss: 0.3181\n",
      "Epoch [9/20], Step [72/75], Loss: 0.2019\n",
      "Epoch [9/20], Step [76/75], Loss: 0.2258\n",
      "Epoch [10/20], Step [4/75], Loss: 0.0017\n",
      "Epoch [10/20], Step [8/75], Loss: 0.1051\n",
      "Epoch [10/20], Step [12/75], Loss: 0.1813\n",
      "Epoch [10/20], Step [16/75], Loss: 0.0509\n",
      "Epoch [10/20], Step [20/75], Loss: 0.0199\n",
      "Epoch [10/20], Step [24/75], Loss: 0.0119\n",
      "Epoch [10/20], Step [28/75], Loss: 0.0142\n",
      "Epoch [10/20], Step [32/75], Loss: 0.0815\n",
      "Epoch [10/20], Step [36/75], Loss: 0.0512\n",
      "Epoch [10/20], Step [40/75], Loss: 0.0023\n",
      "Epoch [10/20], Step [44/75], Loss: 0.0132\n",
      "Epoch [10/20], Step [48/75], Loss: 0.0080\n",
      "Epoch [10/20], Step [52/75], Loss: 0.0540\n",
      "Epoch [10/20], Step [56/75], Loss: 0.0009\n",
      "Epoch [10/20], Step [60/75], Loss: 0.0145\n",
      "Epoch [10/20], Step [64/75], Loss: 0.0004\n",
      "Epoch [10/20], Step [68/75], Loss: 0.1473\n",
      "Epoch [10/20], Step [72/75], Loss: 0.0053\n",
      "Epoch [10/20], Step [76/75], Loss: 0.0002\n",
      "Epoch [11/20], Step [4/75], Loss: 0.0007\n",
      "Epoch [11/20], Step [8/75], Loss: 0.0060\n",
      "Epoch [11/20], Step [12/75], Loss: 0.0028\n",
      "Epoch [11/20], Step [16/75], Loss: 0.0361\n",
      "Epoch [11/20], Step [20/75], Loss: 0.0250\n",
      "Epoch [11/20], Step [24/75], Loss: 0.0337\n",
      "Epoch [11/20], Step [28/75], Loss: 0.0019\n",
      "Epoch [11/20], Step [32/75], Loss: 0.7529\n",
      "Epoch [11/20], Step [36/75], Loss: 0.0448\n",
      "Epoch [11/20], Step [40/75], Loss: 0.1202\n",
      "Epoch [11/20], Step [44/75], Loss: 0.2077\n",
      "Epoch [11/20], Step [48/75], Loss: 0.0253\n",
      "Epoch [11/20], Step [52/75], Loss: 0.0381\n",
      "Epoch [11/20], Step [56/75], Loss: 0.5286\n",
      "Epoch [11/20], Step [60/75], Loss: 0.0577\n",
      "Epoch [11/20], Step [64/75], Loss: 0.0079\n",
      "Epoch [11/20], Step [68/75], Loss: 0.0726\n",
      "Epoch [11/20], Step [72/75], Loss: 0.0376\n",
      "Epoch [11/20], Step [76/75], Loss: 0.0011\n",
      "Epoch [12/20], Step [4/75], Loss: 0.0284\n",
      "Epoch [12/20], Step [8/75], Loss: 0.0030\n",
      "Epoch [12/20], Step [12/75], Loss: 0.6147\n",
      "Epoch [12/20], Step [16/75], Loss: 0.0078\n",
      "Epoch [12/20], Step [20/75], Loss: 0.1138\n",
      "Epoch [12/20], Step [24/75], Loss: 0.0448\n",
      "Epoch [12/20], Step [28/75], Loss: 0.0870\n",
      "Epoch [12/20], Step [32/75], Loss: 0.1784\n",
      "Epoch [12/20], Step [36/75], Loss: 0.0068\n",
      "Epoch [12/20], Step [40/75], Loss: 0.0018\n",
      "Epoch [12/20], Step [44/75], Loss: 0.0213\n",
      "Epoch [12/20], Step [48/75], Loss: 0.0073\n",
      "Epoch [12/20], Step [52/75], Loss: 0.0477\n",
      "Epoch [12/20], Step [56/75], Loss: 0.1201\n",
      "Epoch [12/20], Step [60/75], Loss: 0.0267\n",
      "Epoch [12/20], Step [64/75], Loss: 0.0020\n",
      "Epoch [12/20], Step [68/75], Loss: 0.0003\n",
      "Epoch [12/20], Step [72/75], Loss: 0.0267\n",
      "Epoch [12/20], Step [76/75], Loss: 0.0001\n",
      "Epoch [13/20], Step [4/75], Loss: 0.0105\n",
      "Epoch [13/20], Step [8/75], Loss: 0.0055\n",
      "Epoch [13/20], Step [12/75], Loss: 0.0316\n",
      "Epoch [13/20], Step [16/75], Loss: 0.4874\n",
      "Epoch [13/20], Step [20/75], Loss: 0.0232\n",
      "Epoch [13/20], Step [24/75], Loss: 0.1543\n",
      "Epoch [13/20], Step [28/75], Loss: 0.0038\n",
      "Epoch [13/20], Step [32/75], Loss: 0.0183\n",
      "Epoch [13/20], Step [36/75], Loss: 1.1728\n",
      "Epoch [13/20], Step [40/75], Loss: 0.0205\n",
      "Epoch [13/20], Step [44/75], Loss: 0.2097\n",
      "Epoch [13/20], Step [48/75], Loss: 0.0710\n",
      "Epoch [13/20], Step [52/75], Loss: 0.0698\n",
      "Epoch [13/20], Step [56/75], Loss: 0.0331\n",
      "Epoch [13/20], Step [60/75], Loss: 0.3861\n",
      "Epoch [13/20], Step [64/75], Loss: 0.0172\n",
      "Epoch [13/20], Step [68/75], Loss: 0.7486\n",
      "Epoch [13/20], Step [72/75], Loss: 0.2457\n",
      "Epoch [13/20], Step [76/75], Loss: 0.0134\n",
      "Epoch [14/20], Step [4/75], Loss: 0.1789\n",
      "Epoch [14/20], Step [8/75], Loss: 0.0222\n",
      "Epoch [14/20], Step [12/75], Loss: 0.3162\n",
      "Epoch [14/20], Step [16/75], Loss: 0.1106\n",
      "Epoch [14/20], Step [20/75], Loss: 0.0947\n",
      "Epoch [14/20], Step [24/75], Loss: 0.3622\n",
      "Epoch [14/20], Step [28/75], Loss: 0.5630\n",
      "Epoch [14/20], Step [32/75], Loss: 0.0374\n",
      "Epoch [14/20], Step [36/75], Loss: 0.0005\n",
      "Epoch [14/20], Step [40/75], Loss: 0.2200\n",
      "Epoch [14/20], Step [44/75], Loss: 0.0021\n",
      "Epoch [14/20], Step [48/75], Loss: 0.0048\n",
      "Epoch [14/20], Step [52/75], Loss: 0.0003\n",
      "Epoch [14/20], Step [56/75], Loss: 0.0382\n",
      "Epoch [14/20], Step [60/75], Loss: 0.1943\n",
      "Epoch [14/20], Step [64/75], Loss: 0.0284\n",
      "Epoch [14/20], Step [68/75], Loss: 0.0293\n",
      "Epoch [14/20], Step [72/75], Loss: 0.0003\n",
      "Epoch [14/20], Step [76/75], Loss: 0.0002\n",
      "Epoch [15/20], Step [4/75], Loss: 0.0598\n",
      "Epoch [15/20], Step [8/75], Loss: 0.0010\n",
      "Epoch [15/20], Step [12/75], Loss: 0.0263\n",
      "Epoch [15/20], Step [16/75], Loss: 0.1016\n",
      "Epoch [15/20], Step [20/75], Loss: 0.0602\n",
      "Epoch [15/20], Step [24/75], Loss: 0.0336\n",
      "Epoch [15/20], Step [28/75], Loss: 0.0002\n",
      "Epoch [15/20], Step [32/75], Loss: 0.0246\n",
      "Epoch [15/20], Step [36/75], Loss: 0.0271\n",
      "Epoch [15/20], Step [40/75], Loss: 0.0004\n",
      "Epoch [15/20], Step [44/75], Loss: 0.0002\n",
      "Epoch [15/20], Step [48/75], Loss: 0.0002\n",
      "Epoch [15/20], Step [52/75], Loss: 0.0008\n",
      "Epoch [15/20], Step [56/75], Loss: 0.0282\n",
      "Epoch [15/20], Step [60/75], Loss: 0.0406\n",
      "Epoch [15/20], Step [64/75], Loss: 0.0150\n",
      "Epoch [15/20], Step [68/75], Loss: 0.1786\n",
      "Epoch [15/20], Step [72/75], Loss: 0.0005\n",
      "Epoch [15/20], Step [76/75], Loss: 0.0002\n",
      "Epoch [16/20], Step [4/75], Loss: 0.0001\n",
      "Epoch [16/20], Step [8/75], Loss: 0.0058\n",
      "Epoch [16/20], Step [12/75], Loss: 0.0251\n",
      "Epoch [16/20], Step [16/75], Loss: 0.1682\n",
      "Epoch [16/20], Step [20/75], Loss: 0.0561\n",
      "Epoch [16/20], Step [24/75], Loss: 0.0577\n",
      "Epoch [16/20], Step [28/75], Loss: 0.0143\n",
      "Epoch [16/20], Step [32/75], Loss: 0.0133\n",
      "Epoch [16/20], Step [36/75], Loss: 0.0058\n",
      "Epoch [16/20], Step [40/75], Loss: 0.0021\n",
      "Epoch [16/20], Step [44/75], Loss: 0.0391\n",
      "Epoch [16/20], Step [48/75], Loss: 0.0039\n",
      "Epoch [16/20], Step [52/75], Loss: 0.0001\n",
      "Epoch [16/20], Step [56/75], Loss: 0.0091\n",
      "Epoch [16/20], Step [60/75], Loss: 0.0002\n",
      "Epoch [16/20], Step [64/75], Loss: 0.0003\n",
      "Epoch [16/20], Step [68/75], Loss: 0.0346\n",
      "Epoch [16/20], Step [72/75], Loss: 0.0186\n",
      "Epoch [16/20], Step [76/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [4/75], Loss: 0.0002\n",
      "Epoch [17/20], Step [8/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [12/75], Loss: 0.0020\n",
      "Epoch [17/20], Step [16/75], Loss: 0.0735\n",
      "Epoch [17/20], Step [20/75], Loss: 0.0087\n",
      "Epoch [17/20], Step [24/75], Loss: 0.0684\n",
      "Epoch [17/20], Step [28/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [32/75], Loss: 0.0111\n",
      "Epoch [17/20], Step [36/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [40/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [44/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [48/75], Loss: 0.0001\n",
      "Epoch [17/20], Step [52/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [56/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [60/75], Loss: 0.0002\n",
      "Epoch [17/20], Step [64/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [68/75], Loss: 0.0000\n",
      "Epoch [17/20], Step [72/75], Loss: 0.0008\n",
      "Epoch [17/20], Step [76/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [4/75], Loss: 0.0417\n",
      "Epoch [18/20], Step [8/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [12/75], Loss: 0.0004\n",
      "Epoch [18/20], Step [16/75], Loss: 0.2729\n",
      "Epoch [18/20], Step [20/75], Loss: 0.0068\n",
      "Epoch [18/20], Step [24/75], Loss: 0.0127\n",
      "Epoch [18/20], Step [28/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [32/75], Loss: 0.0114\n",
      "Epoch [18/20], Step [36/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [40/75], Loss: 0.0002\n",
      "Epoch [18/20], Step [44/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [48/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [52/75], Loss: 0.0001\n",
      "Epoch [18/20], Step [56/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [60/75], Loss: 0.0001\n",
      "Epoch [18/20], Step [64/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [68/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [72/75], Loss: 0.0000\n",
      "Epoch [18/20], Step [76/75], Loss: 0.0001\n",
      "Epoch [19/20], Step [4/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [8/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [12/75], Loss: 0.0002\n",
      "Epoch [19/20], Step [16/75], Loss: 0.1330\n",
      "Epoch [19/20], Step [20/75], Loss: 0.0011\n",
      "Epoch [19/20], Step [24/75], Loss: 0.0374\n",
      "Epoch [19/20], Step [28/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [32/75], Loss: 0.0141\n",
      "Epoch [19/20], Step [36/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [40/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [44/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [48/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [52/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [56/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [60/75], Loss: 0.0001\n",
      "Epoch [19/20], Step [64/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [68/75], Loss: 0.0000\n",
      "Epoch [19/20], Step [72/75], Loss: 0.0001\n",
      "Epoch [19/20], Step [76/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [4/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [8/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [12/75], Loss: 0.0003\n",
      "Epoch [20/20], Step [16/75], Loss: 0.1352\n",
      "Epoch [20/20], Step [20/75], Loss: 0.0106\n",
      "Epoch [20/20], Step [24/75], Loss: 0.1090\n",
      "Epoch [20/20], Step [28/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [32/75], Loss: 0.0074\n",
      "Epoch [20/20], Step [36/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [40/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [44/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [48/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [52/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [56/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [60/75], Loss: 0.0001\n",
      "Epoch [20/20], Step [64/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [68/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [72/75], Loss: 0.0000\n",
      "Epoch [20/20], Step [76/75], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training num_epochs = 20\n",
    "# Hàm Loss và Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(newsgroups_train.data)/batch_size)\n",
    "    #  Lặp lại qua từng gói\n",
    "    for i in range(total_batch + 1):\n",
    "        batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()            # Đưa các giá trị gradient về 0\n",
    "        outputs = net(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()                  # Tính toán các gradient  \n",
    "        optimizer.step()                 # Cập nhật các tham số\n",
    "\n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(newsgroups_train.data)//batch_size, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test articles: 79 %\n"
     ]
    }
   ],
   "source": [
    "# Test model num_epochs = 20\n",
    "correct = 0\n",
    "\n",
    "total_test_data = len(newsgroups_test.target)\n",
    "total_batch = int(total_test_data/batch_size)\n",
    "for i in range(total_batch + 1):\n",
    "    batch_x_test,batch_y_test = get_batch(newsgroups_test,i,batch_size)\n",
    "    articles = Variable(torch.FloatTensor(batch_x_test))\n",
    "    labels = torch.LongTensor(batch_y_test)\n",
    "    \n",
    "    outputs = net(articles)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "   \n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the network on the test articles: %d %%' % (100 * correct / total_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
