{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân loại tập văn bản 20newsgroup với Multinomial Naive Bayes và LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Những công cụ được sử dụng:\n",
    "  + Thư viện MultimonialNB và LinearSVC của sklearn. \n",
    "  + Sử dụng TfidfVectorizer (Term Frequency – Inverse Document Frequency) để sinh ra các feature vector\n",
    "- Sử dụng lại tập văn bản đã được xử lý khi làm việc với Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa dữ diệu từ file tập train vào X_train\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train'):\n",
    "    sub_directory_train = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_train):\n",
    "        news_file_path = sub_directory_train + '\\\\' + news_file\n",
    "        with open(news_file_path) as f:\n",
    "            news = f.read()\n",
    "            X_train.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa dữ diệu từ file tập test vào X_test\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test'):\n",
    "    sub_directory_test = 'E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_test' + '\\\\' + i\n",
    "    for news_file in os.listdir(sub_directory_test):\n",
    "        news_file_path = sub_directory_test + '\\\\' + news_file\n",
    "        with open(news_file_path) as f:\n",
    "            news = f.read()\n",
    "            X_test.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa các nhãn lớp (chủ đề) của các file train vào y_train\n",
    "\n",
    "with open('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\train_label.txt') as f :\n",
    "    all_the_lines = f.readlines()\n",
    "    for i in all_the_lines: \n",
    "        i = int(i)\n",
    "        y_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đưa các nhãn lớp (chủ đề) của các file test vào y_test\n",
    "\n",
    "with open('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\test_label.txt') as f :\n",
    "    all_the_lines = f.readlines()\n",
    "    for i in all_the_lines:         \n",
    "        i = int(i)\n",
    "        y_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Làm xáo trộn tập train\n",
    "mapIndexPosition = list(zip(X_train, y_train))\n",
    "random.shuffle(mapIndexPosition)\n",
    "X_train, y_train = zip(*mapIndexPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Làm xáo trộn tập test\n",
    "mapIndexPosition = list(zip(X_test, y_test))\n",
    "random.shuffle(mapIndexPosition)\n",
    "X_test, y_test = zip(*mapIndexPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = list(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7926181625066383\n"
     ]
    }
   ],
   "source": [
    "# Không loại bỏ stopwords và tham số alpha\n",
    "train_NB = Pipeline([ ('vectorizer', TfidfVectorizer()), ('classifier', MultinomialNB())])\n",
    "\n",
    "train_NB.fit(X_train, y_train)\n",
    "print(\"Accuracy: \" + str(train_NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8301911842804036\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ stopwords\n",
    "train_NB = Pipeline([ ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))), ('classifier', MultinomialNB())])\n",
    "\n",
    "train_NB.fit(X_train, y_train)\n",
    "print(\"Accuracy: \" + str(train_NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 5, Accuracy: 0.798194370685077\n",
      "Alpha: 0.5, Accuracy: 0.841343600637281\n",
      "Alpha: 0.05, Accuracy: 0.8583377588953797\n",
      "Alpha: 0.005, Accuracy: 0.8570100902814658\n",
      "Alpha: 0.0005, Accuracy: 0.8490440785979819\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ stopwords kết hợp với tham số alpha\n",
    "for alpha in [5, 0.5, 0.05, 0.005, 0.0005]:\n",
    "    train_NB = Pipeline([ ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))), \n",
    "                          ('classifier', MultinomialNB(alpha = alpha))])\n",
    "\n",
    "    train_NB.fit(X_train, y_train)\n",
    "    print(\"Alpha: \" + str(alpha) + \", Accuracy: \" + str(train_NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8568773234200744\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ tham số alpha = 0.05 cùng với tham số min_df = 5 ( loại bỏ những từ xuất hiện ít hơn 5 lần )\n",
    "train_NB = Pipeline([ ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'), min_df=5)), \n",
    "                          ('classifier', MultinomialNB(alpha = 0.05))])\n",
    "\n",
    "train_NB.fit(X_train, y_train)\n",
    "print(\"Accuracy: \" + str(train_NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_NB = Pipeline([ ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))), \n",
    "                          ('classifier', MultinomialNB(alpha = 0.05))])\n",
    "\n",
    "train_NB.fit(X_train, y_train)\n",
    "\n",
    "labels = []\n",
    "for i in os.listdir('E:\\\\Project 1\\\\20_newsgroups_exercise\\\\20_newsgroup_train_test\\\\20newsgroup_train'):\n",
    "    labels.append(i)\n",
    "y_pred_NB = train_NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.82      0.82       320\n",
      "           comp.graphics       0.77      0.74      0.75       389\n",
      " comp.os.ms-windows.misc       0.75      0.77      0.76       394\n",
      "comp.sys.ibm.pc.hardware       0.73      0.80      0.77       393\n",
      "   comp.sys.mac.hardware       0.86      0.83      0.85       384\n",
      "          comp.windows.x       0.86      0.78      0.82       392\n",
      "            misc.forsale       0.85      0.84      0.84       389\n",
      "               rec.autos       0.92      0.93      0.92       396\n",
      "         rec.motorcycles       0.93      0.97      0.95       398\n",
      "      rec.sport.baseball       0.95      0.94      0.95       398\n",
      "        rec.sport.hockey       0.95      0.97      0.96       400\n",
      "               sci.crypt       0.87      0.96      0.91       396\n",
      "         sci.electronics       0.83      0.76      0.79       392\n",
      "                 sci.med       0.93      0.86      0.90       396\n",
      "               sci.space       0.91      0.93      0.92       395\n",
      "  soc.religion.christian       0.80      0.96      0.87       399\n",
      "      talk.politics.guns       0.76      0.94      0.84       364\n",
      "   talk.politics.mideast       0.97      0.94      0.96       376\n",
      "      talk.politics.misc       0.86      0.75      0.80       310\n",
      "      talk.religion.misc       0.91      0.49      0.64       251\n",
      "\n",
      "                accuracy                           0.86      7532\n",
      "               macro avg       0.86      0.85      0.85      7532\n",
      "            weighted avg       0.86      0.86      0.86      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred_NB, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8749336165693044\n"
     ]
    }
   ],
   "source": [
    "# Không loại bỏ stopwords\n",
    "train_SVC = Pipeline([ ('vectorizer', TfidfVectorizer()), ('classifier', LinearSVC())])\n",
    "\n",
    "train_SVC.fit(X_train, y_train)\n",
    "print(\"Accuracy: \" + str(train_SVC.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8737387148167818\n"
     ]
    }
   ],
   "source": [
    "# Loại bỏ stopwords\n",
    "train_SVC = Pipeline([ ('vectorizer', TfidfVectorizer(stop_words = stopwords.words('English'))), ('classifier', LinearSVC())])\n",
    "\n",
    "train_SVC.fit(X_train, y_train)\n",
    "print(\"Accuracy: \" + str(train_SVC.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVC = train_SVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.84      0.84      0.84       320\n",
      "           comp.graphics       0.76      0.81      0.78       389\n",
      " comp.os.ms-windows.misc       0.79      0.82      0.80       394\n",
      "comp.sys.ibm.pc.hardware       0.76      0.77      0.76       393\n",
      "   comp.sys.mac.hardware       0.87      0.84      0.86       384\n",
      "          comp.windows.x       0.87      0.76      0.81       392\n",
      "            misc.forsale       0.83      0.92      0.87       389\n",
      "               rec.autos       0.93      0.92      0.93       396\n",
      "         rec.motorcycles       0.97      0.97      0.97       398\n",
      "      rec.sport.baseball       0.95      0.96      0.95       398\n",
      "        rec.sport.hockey       0.97      0.97      0.97       400\n",
      "               sci.crypt       0.94      0.94      0.94       396\n",
      "         sci.electronics       0.81      0.81      0.81       392\n",
      "                 sci.med       0.92      0.90      0.91       396\n",
      "               sci.space       0.94      0.93      0.94       395\n",
      "  soc.religion.christian       0.90      0.94      0.92       399\n",
      "      talk.politics.guns       0.81      0.93      0.87       364\n",
      "   talk.politics.mideast       0.98      0.90      0.94       376\n",
      "      talk.politics.misc       0.85      0.78      0.81       310\n",
      "      talk.religion.misc       0.82      0.68      0.74       251\n",
      "\n",
      "                accuracy                           0.87      7532\n",
      "               macro avg       0.87      0.87      0.87      7532\n",
      "            weighted avg       0.88      0.87      0.87      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred_SVC, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
